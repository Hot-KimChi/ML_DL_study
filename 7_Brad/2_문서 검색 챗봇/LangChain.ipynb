{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Learning in Ultrasound\\nImaging\\nThis article provides an overview of use of deep, data-driven learning strategies in\\nultrasound systems, from the front-end to advanced applications. The authors discussthe use of these new computational approaches in all aspects of ultrasound imaging,ranging from ideas that are at the interface of raw signal acquisition (includingadaptive beam forming) and image formatio n, to learning compressive codes for color\\nDoppler acquisition to learning strategie s for performing clutter suppression.\\nBy R UUD J. G. VA N SLOUN ,Member IEEE ,REGEV COHEN ,Graduate Student Member IEEE ,\\nAND YONINA C. E LDAR ,Fellow IEEE\\nABSTRACT |In this article, we consider deep learning\\nstrategies in ultrasound systems, from the front end to\\nadvanced applications. Our goal is to provide the reader with\\na broad understanding of the possible impact of deep learning\\nmethodologies on many aspects of ultrasound imaging.\\nIn particular, we discuss methods that lie at the interfaceof signal acquisition and machine learning, exploiting both\\ndata structure (e.g., sparsity in some domain) and data\\ndimensionality (big data) already at the raw radio-frequencychannel stage. As some examples, we outline efﬁcient and\\neffective deep learning soluti ons for adaptive beamforming\\nand adaptive spectral Doppler through artiﬁcial agents, learn\\ncompressive encodings for the color Doppler, and provide\\na framework for structured signal recovery by learning fastapproximations of iterative minimization problems, with\\napplications to clutter suppression and super-resolution\\nultrasound. These emerging technologies may have aconsiderable impact on ultrasound imaging, showing promise\\nacross key components in the receive processing chain.\\nKEYWORDS |Beamforming; compression; deep learning; deep\\nunfolding; Doppler; image reconstruction; super resolution;ultrasound imaging.\\nManuscript received April 11, 2019; revised June 24, 2019 and July 25, 2019;\\naccepted July 27, 2019. Date of publication August 21, 2019; date of current\\nversion December 26, 2019. (Corresponding author: Ruud J. G. van Sloun.)\\nR. J. G. van Sloun is with the Department of Electrical Engineering, Eindhoven\\nUniversity of Technology, 5600 MB Eindhoven, The Netherlands (email:r.j.g.v.sloun@tue.nl).\\nR. Cohen is with the Department of Electrical Engineering, Technion, Haifa,\\nIsrael.\\nY. C . E l da r is with the Faculty of Mathematics and Computer Science,\\nWeizmann Institute of Science, Rehovot 7610001, Israel (e-mail:\\nyonina.eldar@weizmann.ac.il).\\nDigital Object Identiﬁer 10.1109/JPROC.2019.2932116I.INTRODUCTION\\nDiagnostic imaging plays a critical role in healthcare,\\nserving as a fundamental asset for timely diagnosis,\\ndisease staging, and management, as well as for treatment\\nchoice, planning, guidance, and follow-up. Among the\\ndiagnostic imaging options, ultrasound imaging [1] isuniquely positioned, being a highly cost-effective modality\\nthat offers the clinician an unmatched and invaluable\\nlevel of interaction, enabled by its real-time nature.Its portability and cost effectiveness permit point-of-care\\nimaging at the bedside, in emergency settings, rural clinics,\\nand developing countries. Ultrasonography is increasingly\\nused across many medical specialties, spanning from\\nobstetrics to cardiology and oncology , and its market shareis globally growing.\\nOn the technological side, ultrasound probes are\\nbecoming increasingly compact and portable, with themarket demand for low-cost “pocket-sized” devices\\nexpanding [2], [3]. Transducers are miniaturized, allow-\\ning, e.g., in-body imaging for interventional applica-\\ntions. At the same time, there is a strong trend toward\\n3-D imaging [4] and the use of high-frame-rate imagingschemes [5]; both accompanied by dramatically increasing\\ndata rates that pose a heavy burden on the probe-system\\ncommunication and subsequent image reconstructionalgorithms. Systems today offer a wealth of advanced\\napplications and methods, including shear wave elasticity\\nimaging (SWEI) [6], ultrasensitive Doppler [7], and ultra-\\nsound localization microscopy (ULM) for super-resolution\\nmicrovascular imaging [8].\\nWith the demand for high-quality image reconstruction\\nand signal extraction from unfocused planar wave trans-\\nmissions that facilitate fast imaging, and a push toward\\n0018-9219 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.\\nSee http://www.ieee.org/publications_standards/publi cations/rights/index.html for more information.\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 11van Sloun et al. : Deep Learning in Ultrasound Imaging\\nminiaturization, modern ultrasound imaging leans heavily\\non innovations in powerful receive channel processing.\\nIn this article, we discuss how artiﬁcial intelligence and\\ndeep learning methods can play a compelling role in thisprocess and demonstrate how these data-driven systems\\ncan be leveraged across the ultrasound imaging chain.\\nWe aim to provide the reader with a broad understandingof the possible impact of deep learning on a variety of\\nultrasound imaging aspects, placing particular emphasis\\non methods that exploit both the power of data and\\nsignal structure (for instance sparsity in some domain)\\nto yield robust and data-efﬁcient solutions. We believethat methods that exploit models and structure together\\nwith learning from data can pave the way to interpretable\\nand powerful processing methods from limited trainingsets. As such, throughout this article, we will typically\\nﬁrst discuss an appropriate model-based solution for the\\nproblems considered and then follow by a data-driven deep\\nlearning solution derived from it.\\nWe start by brieﬂy describing a standard ultrasound\\nimaging chain in Section II. We then elaborate on several\\ndedicated deep learning solutions that aim at improv-\\ning key components in this processing pipeline, cover-ing adaptive beamforming (see Section III-A), adaptive\\nspectral Doppler (see Section III-B), compressive tissue\\nDoppler (see Section III-C), and clutter suppression (see\\nSection III-D). In Section IV, we show how the syner-\\ngetic exploitation of deep learning and signal structureenables robust super-resolution microvascular ultrasound\\nimaging. Finally , we discuss future perspectives, opportu-\\nnities, and challenges for the holistic integration of artiﬁ-cial intelligence and deep learning methods in ultrasound\\nsystems.\\nII. ULTRASOUND IMAGING CHAIN\\nAT A G L A N C E\\nA. Transmit Schemes\\nThe resolution, contrast, and overall ﬁdelity of ultra-\\nsound pulse–echo imaging relies on careful optimization\\nacross its entire imaging chain. At the front end, imaging\\nstarts with the design of appropriate transmit schemes.\\nAt this stage, crucial tradeoffs are made, in which the\\nframe rate, imaging depth, and attainable axial and lat-eral resolution are weighted c arefully against each other:\\nimproved resolution can be achieved through the use of\\nhigher pulse modulation frequencies; yet, these shorterwavelengths suffer from increased absorption and thus\\nlead to reduced penetration depth. Likewise, a high frame\\nrate can be reached by exploiting parallel transmissionschemes based on, e.g., planar or diverging waves. How-\\never, use of such unfocused transmissions comes at the\\ncost of loss in lateral resolution compared to line-based\\nscanning with tightly focused beams. As such, optimal\\ntransmit schemes depend on the application.\\nToday , an increasing amount of ultrasound applica-\\ntions rely on high-frame-rate (dubbed ultrafast) imaging.Among these are, e.g., ULM (see Section IV), highly sen-\\nsitive Doppler, and shear wave elastography , where the\\nformer two mostly exploit the incredible vastness of data\\nto obtain accurate signal statistics, and the latter lever-ages high-speed imaging to track ultrasound-induced shear\\nwaves propagating at several meters per second.\\nWith the expanding use of ultrafast transmit sequences\\nin modern ultrasound imaging, a strong burden is placed\\non the subsequent receive channel processing. High\\ndata rates not only raise substantial hardware complica-\\ntions related to power consumption, data storage, and\\ndata transfer; the corresponding unfocused transmissionsrequire much more advanced receive beamforming and\\nclutter suppression to reach satisfactory image quality .\\nB. Receive Processing, Sampling, and\\nBeamforming\\nModern receive channel processing is shifting toward\\nthe digital domain, relying on computational power\\nand very-high-bandwidth communication channels to\\nenable advanced digital parallel (pixel-based) beamform-\\ning and coherent compounding across multiple trans-mit/receive events. For large channel counts, e.g., in dense\\nmatrix probes that facilitate high-resolution 3-D imaging,\\nthe number of coaxial cables required to connect all probeelements to the back-end system quickly becomes infea-\\nsible. To address this, dedicated switching and process-\\ning already take place in the probe head, e.g., in the\\nform of multiplexing or microbeamforming. Slow-time\\n1\\nmultiplexing distributes the received channel data across\\nmultiple transmits, by only communicating a subset of the\\nnumber of channels to the back end for each such trans-\\nmit. This consequently reduces the achieved frame rate.In microbeamforming, an analog prebeamforming step is\\nperformed to compress channel data from multiple (adja-\\ncent) elements into a single focused line. This, however,\\nimpairs ﬂexibility in subsequent digital beamforming, lim-\\niting the achievable image quality . Other approaches aimat mixing multiple channels through analog modulation\\nwith chipping sequences [9]. Additional analog process-\\ning includes signal ampliﬁcation by a low-noise ampli-ﬁer (LNA) as well as depth (i.e., fast time) dependent gain\\ncompensation (TGC) for attenuation correction.\\nDigital receive beamforming in ultrasound imaging is\\ndynamic, i.e., receive focusing is dynamically optimized\\nbased on the scan depth. The industry standard is delay-and-sum beamforming, where depth-dependent channel\\ntapering (or apodization) is optimized and ﬁne-tuned\\nbased on the system and application. Delay-and-sumbeamforming is commonplace due to its low complexity ,\\nproviding real-time image reconstruction, albeit at a high\\nsampling rate and nonoptimal image quality .\\n1In ultrasound imaging, we make a distinction between slow time\\nand fast time: slow time refers to a sequence of snapshots (i.e., acrossmultiple transmit/receive events) at the pulse repetition rate, whereas\\nfast time refers to samples along depth.\\n12 P\\nROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nPerforming beamforming in the digital domain requires\\nsampling the signals received at the transducer elements\\nand transmitting the samples to a back-end processing\\nunit. To achieve sufﬁcient delay resolution for focusing,the received signals are typically sampled at four to\\nten times their bandwidth, i.e., the sampling rate may\\nseverely exceed the Nyquist rate. A possible approachfor sampling rate reduction is to consider the received\\nsignals within the framework of the ﬁnite rate of innova-\\ntion (FRI) [10], [11]. Tur et al. [12] modeled the received\\nsignal at each element as a ﬁnite sum of replicas of\\nthe transmitted pulse backscattered from reﬂectors. Thereplicas are fully described by their unknown amplitudes\\nand delays, which can be recovered from the signals’\\nFourier series coefﬁcients. The latter can be computedfrom low-rate samples of the signal using compressed\\nsensing (CS) techniques [10], [13]. Wagner et al. [14]\\nand Wagner et al. [15] extended this approach and\\nintroduced compressed beamforming. It was shown that\\nthe beamformed signal follows an FRI model, and thus,it can be reconstructed from a linear combination of\\nthe Fourier coefﬁcients of the received signals. Moreover,\\nthese coefﬁcients can be obtained from low-rate sam-ples of the received signals taken according to the Xam-\\npling framework [16]–[18]. Chernyakova and Eldar [3]\\nshowed that this Fourier domain relationship between\\nthe beam and the received signals holds irrespective of\\nthe FRI model. This leads to a general concept of thefrequency-domain beamforming (FDBF) [3] that is equiv-\\nalent to beamforming in time. FDBF allows sampling the\\nreceived signals at their effective Nyquist rate withoutassuming a structured model; thus, it avoids the oversam-\\npling dictated by digital implementation of beamforming\\nin time. Furthermore, when assuming that the beam obeys\\nan FRI model, the received signals can be sampled at\\nsub-Nyquist rates, leading to up to 28-fold reduction in thesampling rate [19]–[21].\\nC. B-Mode, M-Mode, and Doppler\\nUltrasound imaging provides anatomical information\\nthrough the so-called brightness mode (B-mode). B-mode\\nimaging is performed by envelope-detecting the beam-\\nformed signals, e.g., through the calculation of the mag-nitude of the complex in-phase and quadrature (IQ) data.\\nFor visualization purposes, the dynamic range of these\\nenvelope-detected signals is subsequently compressed viaa logarithmic transformation or speciﬁcally designed com-\\npression curves based on a lookup table. Scan conversion\\nthen maps these intensities to the desired (Cartesian)pixel coordinate system. The visualization of a single\\nB-mode scan line (i.e., brightness over fast time) across\\nmultiple transmit–receive events (i.e., slow time), is called\\nmotion-mode (M-mode) imaging.\\nBeyond anatomical information, ultrasound imaging\\nalso permits the measurement of functional parame-\\nters related to blood ﬂow and tissue displacement. Theextraction of such velocity signals is called Doppler\\nprocessing. We distinguish between two types of veloc-\\nity estimators: color Doppler and spectral Doppler. Color\\nDoppler provides an estimate of the mean velocity throughthe evaluation of the ﬁrst lag of the autocorrelation func-\\ntion for a series of snapshots across slow time [22]. Spec-\\ntral Doppler provides the entire velocity distribution in aspeciﬁed image region through estimation of the full power\\nspectral density and visualizes its evolution over time in a\\nspectrogram [23]. Spectral Doppler methods are relevant\\nfor, e.g., detecting turbulent ﬂow in stenotic arteries or\\nacross heart valves. Besides assessing blood ﬂow, Dopplerprocessing also ﬁnds applications in measurement of tissue\\nvelocities (tissue Doppler), e.g., for assessment of the\\nmyocardial strain.\\nD. Advanced Applications\\nIn addition to B-mode, M-mode, and Doppler scan-\\nning, ultrasound data are used in a number of advancedapplications. For instance, elastography methods aim at\\nmeasuring mechanical parameters related to tissue elas-\\nticity and rely on the analysis of displacements followingsome form of imposed stress. Stress may be delivered\\nmanually (through gentle pushing), naturally (e.g., in the\\nmyocardium of a beating heart) or acoustically , as done\\nin acoustic radiation force impulse imaging (ARFI) [24].\\nAlternatively , the speed of laterally traveling shear wavesinduced by an acoustic push-pulse can be measured, with\\nthis speed being directly related to the shear modulus [6].\\nSWEI also permits measurement of tissue viscosity inaddition to stiffness through the assessment of wave\\ndispersion [25]. All the above-mentioned methods rely\\non the adequate measurement of local tissue velocity\\nor displacement through some form of tissue Doppler\\nprocessing.\\nWhile Doppler methods enable estimation of blood\\nﬂow, detection of low-velocity microvascular ﬂow is chal-\\nlenging since its Doppler spectrum overlaps with thatof the strong tissue clutter. Contrast-enhanced ultra-\\nsound (CEUS) permits visualization and characterization\\nof microvascular perfusion through the use of gas-ﬁlled\\nmicrobubbles [26], [27]. These intravascular bubbles are\\nsized similar to red blood cells, reaching the smallestcapillaries in the vascular net, and exhibit a particular non-\\nlinear response when insoniﬁed. The latter is speciﬁcally\\nexploited in contrast-enhanced imaging schemes that aimat isolating this nonlinear response through the dedicated\\npulse sequences. Unfortunately , this does not lead to com-\\nplete tissue suppression since the tissue itself also gener-ates harmonics [28]. Thus, clutter rejection algorithms are\\nbecoming increasingly popular, in particular when used in\\nconjunction with ultrafast imaging [29].\\nRecent developments also leverage the microbubbles\\nused in CEUS to yield super-resolution imaging [30]–[33].ULM is a particularly popular approach to achieve this [8].\\nULM methods rely on adequate detection, isolation,\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 13van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 1. Overview of the ultrasound imaging chain, along with the deep learning solutions discussed in this article. Note that, today, analog\\nprocessing at the front end typically comprises some form of lossy (micro-)beamforming to reduce data rates, in contrast to the hereadvocated paradigm based on compressive sub-Nyquist sampling, intelligent ASICs with neural edge computing, and subsequent remotedeep-learning-based processing of the low-rate channel data.\\nand localization of the microbubbles, typically achieved\\nthrough precisely tuned tissue clutter suppression algo-\\nrithms and by posing strong constraints on the allowableconcentrations. We will further elaborate on this approach\\nand its limitations in Section IV , where we discuss a ded-\\nicated deep learning solution for super-resolution ultra-ound that aims at addressing some of these disadvantages.\\nIII. DEEP LEARNING FOR\\n(FRONT -END) ULTRASOUNDPROCESSING\\nThe effectiveness of ultrasound imaging and its applica-\\ntions are dictated by adequate front-end beamforming,\\ncompression, signal extraction (e.g., clutter suppression),\\nand velocity estimation. In this section, we demon-\\nstrate how neural networks, being universal function\\napproximators [34], can learn to act as powerful artiﬁcialagents and signal processors across the imaging chain\\nto improve resolution and contrast, adequately suppress\\nclutter, and enhance spectral estimation. We here referto artiﬁcial agents [35] whenever these learned networks\\nimpact the processing chain by actively and adaptively\\nchanging the settings or parameters of a particular proces-\\nsor depending on the context.\\nDeep learning is the process of learning a hierarchy\\nof parameterized nonlinear transformations (or layers),\\nsuch that it performs a desired function. These elementary\\nnonlinear transformations in a deep network can takemany forms and may embed structural priors. A popular\\nexample of the latter is the translational invariance in\\nimages that is exploited by convolutional neural networks,but we will see that in fact many other structural priors can\\nbe exploited.\\nThe methods proposed throughout this article are both\\nmodel based and learn from data. We complement this\\napproach with ap r i o r i knowledge on the signal structure\\nto develop deep learning models that are both effective\\nand data efﬁcient, i.e., “fast learners.” An overview isgiven in Fig. 1. We assume that the reader is familiar\\nwith the basics of (deep) neural networks. For a general\\nintroduction to deep learning, we refer the reader to [36].\\nA. Beamforming\\n1) Deep Neural Networks as Beamformers: The low\\ncomplexity of delay-and-sum beamforming has made it\\nthe industry standard and commonplace for real-time\\nultrasound beamforming. There are, however, a numberof factors that cause deteriorated reconstruction quality\\nof this naive spatial ﬁltering strategy . First, the chan-\\nnel delays for time-of-ﬂight correction are based on thegeometry of the scene and assume a constant speed\\nof sound across the medium. As a consequence, varia-\\ntions in the speed of sound and the resulting aberra-\\ntions impair proper alignment of echoes stemming from\\nthe same scatterer [37]. Second, the ap r i o r i determined\\nchannel weighting (apodization) of pseudo-aligned echoes\\nbefore summation requires a tradeoff between mainlobe\\nwidth (resolution) and sidelobe level (leakage) [38].\\nDelay-and-sum beamformers are typically hand-tailored\\nbased on knowledge of the array geometry and medium\\nproperties, often including speciﬁcally designed array\\napodization schemes that may vary across imaging depth.\\nInterestingly , it is possible to learn the delays andapodizations from paired channel-image data through\\ngradient-descent by dedicated “delay layers” [39]. To show\\nthis, unfocused channel data were obtained from echocar-diography of six patients for both single- and multiple-line\\nacquisitions. While the latter allows for increased frame\\nrates, it leads to deteriorated image quality when apply-ing standard delay-and-sum beamforming. The authors,\\ntherefore, propose to train a more appropriate delay-and-\\nsum beamforming chain that takes multi-line channel data\\nas an input and produces beamformed images that are\\nas close as possible to those obtained from single-lineacquisitions, minimizing their\\n\\x021distance. Since the intro-\\nduced delay and apodization layers are differentiable,\\n14 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nefﬁcient learning is enabled through backpropagation.\\nAlthough such an approach potentially enables discovery\\nof a more optimal set of parameters dedicated to each\\napplication, the fundamental problem of having ap r i o r i -\\ndetermined static delays and weights remains.\\nSeveral other data-driven beamforming methods have\\nrecently been proposed. In contrast to [39], these aremostly based on “general-purpose” deep neural networks,\\nsuch as stacked autoencoders [40], encoder–decoder\\narchitectures [41], and fully convolutional networks,\\nthat map predelayed channel data to beamformed\\noutputs [42]. In the latter, a 29-layer convolutional net-work was applied to a 3-D stack of array response vectors\\nfor all lateral positions and a set of depths to yield a beam-\\nformed IQ output for those lateral positions and depths.Others exploit neural networks to process channel data in\\nthe Fourier domain [43]. To that end, axially gated sec-\\ntions of the predelayed channel data ﬁrst undergo discrete\\nFourier-transformation. For each frequency bin, the array\\nresponses are then processed by a separate fully connectednetwork. The frequency spectra are subsequently inverse\\nFourier transformed and summed across the array to yield\\na beamformed radio-frequency (RF) signal associated withthat particular axial location. The networks were specif-\\nically trained to suppress off-axis responses (outside the\\nﬁrst nulls of the beam) from the simulations of ultrasound\\nchannel data for point targets.\\nBeyond beamforming for suppression of off-axis\\nscattering, Hyun et al. [44] propose deep convolutional\\nneural networks for joint beamforming and speckle reduc-\\ntion. Rather than applying the latter as a post-processingtechnique, it is embedded in the beamforming process\\nitself, permitting exploitation of both channel and phase\\ninformation that is otherwise irreversibly lost. The net-\\nwork was designed to accept 16 beamformed subaperture\\nRF signals as an input, and outputs speckle-reduced B-mode images. The ﬁnal beamformed images exhibit com-\\nparable speckle-reduction as postprocessed delay-and-sum\\nimages using the optimized Bayesian nonlocal meansalgorithm [45], yet at an improved resolution. Addi-\\ntional applications of deep learning in this con-\\ntext include removal of artifacts in time-delayed and\\nphase-rotated element-wise IQ data in multi-line acquisi-\\ntions for high-frame-rate imaging [46] and synthesizingmultiple-focus images from single-focus images through\\ngenerative adversarial networks [47]. In [48], such gen-\\nerative adversarial networks were used for joint beam-forming and segmentation of cyst phantoms from unfo-\\ncused RF channel data acquired after a single plane-wave\\ntransmission.\\nWhile the ﬂexibility and capacity of very deep neural\\nnetworks in principle allow for learning context-adaptive\\nbeamforming schemes, such highly overparameterized net-\\nworks notoriously rely on vast RF channel data to yield\\nrobust inference under a wide range of conditions. More-over, large networks have a large memory footprint, com-\\nplicating resource-limited implementations.2) Leveraging Model-Based Algorithms: One approach for\\nconstraining the solution space while explicitly embed-\\nding adaptivity is to borrow concepts from model-based\\nadaptive beamforming methods. These techniques steeraway from the ﬁxed-weight presumption and calculate an\\narray apodization depending on the measured signal statis-\\ntics. In the case of pixel-based reconstruction, apodizationweights can be adaptively optimized per pixel. A popular\\nadaptive beamforming method is the minimum variance\\ndistortionless response (MVDR), or Capon, beamformer,\\nwhere optimal weights are deﬁned as those that minimize\\nsignal variance/power, while maintaining distortionlessresponse of the beamformer in the desired source direc-\\ntion. This amounts to solving\\nˆw=a r g m i n\\nwwHRxw\\ns.t. wHa=1 (1)\\nwhere Rxdenotes the covariance matrix calculated over\\nthe receiving array elements and ais a steering vector.\\nWhen the receive signals are already time-of-ﬂight cor-\\nrected, ais a unity vector.\\nSolving (1)involves the inversion of Rx,w h o s ec o m -\\nputational complexity grows cubically with the numberof array elements [50]. To improve stability , it is often\\ncombined with subspace selection through eigendecom-\\nposition, further increasing the computational burden.Another problem is the accurate estimation of R\\nx,t y p i c a l l y\\nrequiring some form of averaging across subarrays and the\\nfast- and slow-time scales. While this implementation of\\nMVDR beamforming is impractical for typical ultrasound\\narrays (e.g., 256 elements) or matrix transducers (e.g.,\\n64×64elements), it does provide a framework in which\\ndeep neural networks can be leveraged efﬁciently and\\neffectively .\\nInstead of attempting to replace the beamforming\\nprocess entirely , a neural network can be used speciﬁcally\\nto act as an artiﬁcial agent that calculates the optimal\\napodization weights wfor each pixel, given the received\\npredelayed channel signals at the array . By only replacingthis bottleneck component in the MVDR beamformer, and\\nconstraining the problem further by promoting close-to-\\ndistortionless response during training (i.e.,\\nΣiwi≈1),\\nthis solution is highly data efﬁcient, interpretable, and\\nhas the ability to learn powerful models from only a few\\nimages [49].\\nThe neural network proposed in [49] is compact, con-\\nsisting of four fully connected layers comprising 128 nodesfor the input and output layers and 32 nodes for the hidden\\nlayers. This dimensionality reduction enforces a compact\\nrepresentation of the data, mitigating the impact of noise.Between every fully connected layer, dropout is applied\\nwith a probability of 0.2. The input of the network is the\\npredelayed (focused) array response for a particular pixel\\n(i.e., a vector of length\\nN,w i t h Nbeing the number of\\narray elements), and its outputs are the corresponding\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 15van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 2. (a) Flow charts of standard delay-and-sum beamforming using ﬁxed apodization weights and (b) adaptive beamforming by deep\\nlearning [49], along with (c) and (d) illustrative reconstructed images (in silico and in vivo) for both methods, respectively. Adaptivebeamforming by deep learning achieves notably better contras t and resolution and generalizes very well to unseen data sets.\\narray apodizations w. This apodization is subsequently\\napplied to the network inputs to yield a beamformed pixel.\\nSince pixels are processed independently by the network,a large amount of training data is available per acquisi-\\ntion. Inference is fast, and real-time rates are achievable\\non a GPU-accelerated system. For an array of 128 ele-ments, adaptive calculation of a set of apodization weights\\nthrough MVDR requires\\n>N3(= 2 097 152) ﬂoating point\\noperations (FLOPS), while the deep-learning architecture\\nonly requires 74 656 FLOPS [49], leading to a more\\nthan 400 ×speedup in the reconstruction time. Additional\\ndetails regarding the adopted network and training strat-\\negy are given in Section III-A3.\\nFig. 2 exempliﬁes the effectiveness of this approach\\non plane-wave ultrasound acquisitions obtained using a\\nlinear array transducer. Compared to standard delay-and-\\nsum, adaptive beamforming with a deep network serving\\nas an artiﬁcial agent visually provides reduced clutter\\nand enhanced tissue contrast. Quantitatively , it yields aslightly elevated contrast-to-noise ratio (CNR) (10.96 ver-\\nsus 11.48 dB), along with signiﬁcantly improved resolution\\n(0.43 versus 0.34 mm, and 0.85 versus 0.70 mm in theaxial and lateral directions, respectively).\\nInterestingly , the neural network exhibits increased sta-\\nbility and robustness compared to the MVDR weight esti-\\nmator. This can be attributed to its small bottleneck latent\\nspace, enforcing apodization weight realizations that arerepresented in a compact basis.\\n3) Design and Training Considerations: The large\\ndynamic range and modulated nature of RF ultrasoundchannel data motivate the use of speciﬁc nonlinear acti-\\nvation functions. While rectiﬁed linear units (ReLUs) are\\ntypically used in image processing, popular for their sparsi-fying nature and ability to avoid vanishing gradients due to\\ntheir positive unbounded output, it inherently causes many“dying nodes” (neurons that do no longer update since\\ntheir gradient is zero) for ultrasound channel data, as a\\nReLU does not preserve (the abundant) negative values.To circumvent this, a hyperbolic tangent function could be\\nused. Unfortunately , the large dynamic range of ultrasound\\nsignals makes it difﬁcult to be in the “sweet spot,” wheregradients are sufﬁciently large, thereby avoiding vanishing\\ngradients during backpropagation across multiple layers.\\nA powerful alternative that is, by nature, unbounded\\nand preserves both positive and negative values is the class\\nof concatenated ReLUs [51]. A particular case is the anti-rectiﬁer function\\nf(x)=\\n/BC/BS\\n/CW\\nx−¯x\\n\\x02x−¯x\\x022\\n/CX\\n+/CW\\n−x−¯x\\n\\x02x−¯x\\x022\\n/CX\\n+\\n/BD/BT (2)\\nwhere [·]+=max(·,0)is the positive part operator, xis\\na vector containing the linear responses of all neurons\\n(before activation) at a particular layer, and ¯xis its mean\\nvalue across all those neurons. The anti-rectiﬁer does notsuffer from vanishing gradients, nor does it lead to dying\\nnodes for negative values, yet provides the nonlinearity\\nthat facilitates learning complex models and representa-tions. This dynamic-range preserving activation scheme is,\\ntherefore, well suited for processing RF or IQ-demodulated\\nultrasound channel data and is also used for the results\\npresented in Fig. 2. These advantages come at the cost of a\\nhigher computational complexity compared to a standardReLU activation.\\nWhen training a neural-network-based ultrasound\\nbeamforming algorithm, it is important to consider theimpact of subsequent signal transformations in the process-\\ning chain. In particular, envelope-detected beamformed\\nsignals typically undergo signiﬁcant dynamic range com-\\npression (e.g., through a logarithmic transformation) to\\n16 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 3. Adaptive spectral Doppler processing using deep learning, displaying (a) illustrative overview of the method, comprising an\\nartiﬁcial agent that adaptively sets the optimal matched ﬁlterbank weights according to the input data, and (b) and (c) spectral estimatesusing Welch’s method and the deep learning approach, respectively. The input was phantom data for the arteria femoralis, with spectraestimated from a coherent processing interval of 64 slow-time samples.\\nproject the high dynamic range of the backscattered ultra-\\nsound signals onto the limited dynamic range of a displayand allow for improved interpretation and diagnostics.\\nTo incorporate this aspect in the neural network’s training\\nloss, beamforming errors can be transformed to attain a\\nmean squared logarithmic error\\nL=\\x03log10([ˆy]+)−log10([y]+)|2\\n2\\n+|log10([−ˆy]+)−log10([−y]+)\\x032\\n2(3)\\nwhere ˆyis a vector containing the neural-network-based\\nprediction of the beamformed responses for all pixels\\nand ycontains the target beamformed signals. For our\\nmodel-based adaptive beamforming solution [49], ycon-\\ntains the MVDR beamformer outputs for each pixel, and\\nˆyis the corresponding set of pixel responses after appli-\\ncation of the apodization weights calculated by the neural\\nnetwork.\\nB. Adaptive Spectral Estimation for Spectral\\nDoppler\\nAs mentioned in Section II, beamformed ultrasound sig-\\nnals are not only used to visualize anatomical informationin B-mode but also permit the extraction of velocities by\\nprocessing subsequent frames across slow time.\\nSpectral Doppler ultrasound enables measurement of\\nblood (and tissue) velocity distributions through the gen-\\neration of a Doppler spectrogram from slow-time data\\nsequences, i.e., a series of subsequent pulse–echo snap-\\nshots. In commercial systems, spectra are estimated using\\nthe Fourier-transform-based periodogram methods, e.g.,the standard Welch approach. Such techniques, however,\\nrequire long observation windows (denoted as “coherentprocessing intervals”) to achieve high spectral resolution\\nand mitigate spectral leakage. This deteriorates the tempo-ral resolution.\\nData-adaptive spectral estimators alleviate the strong\\ntime–frequency resolution tradeoff, providing superior\\nspectral estimates and resolution for a given temporal\\nresolution [52]. The latter is determined by the coherentprocessing interval, which is, in turn, deﬁned by the pulse\\nrepetition frequency and the number of slow-time snap-\\nshots required for a spectral e stimate. Adaptive approaches\\nsteer away from the standard periodogram methods and\\nrely on the content-matched ﬁlterbanks. The ﬁlter coefﬁ-\\ncients for each frequency of interest\\nωare adaptively tuned\\nto, e.g., minimize signal energy while being constrained to\\nunity frequency response. This Capon spectral estimator isgiven by solving [52]\\nˆwω=a r g m i n\\nwωwH\\nωRywω\\ns.t. wH\\nωeω=1 (4)\\nwhere Ryis the covariance matrix of the (slow-time)\\ninput signal vector y,a n d eωis the corresponding Fourier\\nvector. While this adaptive spectral estimator indeedimproves upon standard approaches and signiﬁcantly low-\\ners the required observation window while gaining spectral\\nﬁdelity , it, unfortunately , suffers from high computationalcomplexity stemming from the need for inversion of the\\nsignal covariance matrix.\\nAs for the MVDR beamformer (see Section III-A),\\nwe here demonstrate that neural networks can also\\nbe exploited to provide fast estimators for the optimalmatched ﬁlter coefﬁcients, acting as an artiﬁcial agent.\\nAn overview of this approach is given in Fig. 3, for\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 17van Sloun et al. : Deep Learning in Ultrasound Imaging\\npulsed-wave phantom data for the arteria femoralis [53].\\nThe neural network takes a beamformed slow-time RF\\nsignal as input and outputs a set of ﬁlter coefﬁcients for\\neach ﬁlter in the ﬁlterbank. The slow-time input signalis then passed through this ﬁlterbank to attain a spec-\\ntral estimate. The neural network is trained by mini-\\nmizing the mean squared logarithmic error (3)between\\nthe resulting spectrum and the output spectrum of the\\nhigh-quality adaptive Capon spectral estimator. It com-\\nprised 128 four-layer fully connected subnetworks, each\\nof those predicting the coefﬁcients for one of the 128 ﬁl-\\nters in the ﬁlterbank. The optimization problem is thenregularized by penalizing deviations from unity frequency\\nresponse (4). The length of the slow-time observation\\nwindow was only 64 samples, taken from a single depthsample. Compared to Welch’s periodogram-based method,\\nadaptive spectral estimation by deep learning achieves\\nfar less spectral leakage and higher spectral resolution\\n[see Fig. 3(b) and (c)].\\nTraining the artiﬁcial agent is subject to similar con-\\nsiderations outlined in Section III-A3. First, slow-time\\ninput samples have a large dynamic range, such that a\\nnonsaturating activation scheme is preferred, as in (2).\\nSecond, Doppler spectra are typically presented in deci-\\nbels, advocating for the use of a log-transformed training\\nloss, as in (3). Third, training is regularized by adding\\nan additional loss to penalize predicted ﬁlterbanks that\\ndeviate from unity frequency response.\\nThe above-mentioned approach is designed to processes\\nuniformly sampled slow-time signals. In practice, there is a\\ndesire to expand these techniques to estimators that havethe ability to cope with “gaps,” or even sparsely sampled\\nsignals, since the spectral Doppler processing is typically\\ninterleaved with B-mode imaging for navigation purposes\\n(Duplex mode). To that end, extensions of data-adaptive\\nestimators for periodically gapped data [54] and recoveryfor nested slow-time sampling [55] can be used.\\nC. Compressive Encodings for Tissue Doppler\\nFrom a hardware perspective, a signiﬁcant challenge for\\nthe design of ultrasound devices and transducers is coping\\nwith the limited cable bandwidth and related connectiv-\\nity constraints [56]. This is particularly troublesome forcatheter transducers used in the interventional applica-\\ntions (e.g., intravascular ultrasound or intracardiac echog-\\nraphy), where data need to pass through a highly restrictednumber of cables. While this is less of a concern for\\ntransducers with only few elements, the number of trans-\\nducer elements have expanded greatly in recent devices tofacilitate high-resolution 2-D or 3-D imaging [57]. Beyond\\nthe limited capacity of miniature devices, (future) wire-\\nless transducers will pose similar constraints on data\\nrates [58]. Today , front-end connectivity and bandwidth\\nchallenges are addressed through, e.g., application-speciﬁcintegrated circuits that perform microbeamforming [59] or\\nsimple summation of the receive signals across neighboringelements [60] to compress the full channel data into a\\nmanageable amount, and multiplexing of the receive sig-\\nnals. This inherently entails information loss and typically\\nleads to reduced image quality .\\nInstead of Nyquist-rate sampling of pre-beamformed\\nand multiplexed channel data, compressive sub-Nyquist\\nsampling methods permit reduced-rate imagingwithout sacriﬁcing quality [3], [19]. After (reduced-\\nrate) digitization, additional compression may be\\nachieved through neural networks that serve as\\napplication-speciﬁc encoders. Advances in low-power\\nneural edge computing may permit placing such a trainedencoder at the probe, further alleviating probe-scanner\\ncommunication, and a subsequent high-end decoder at\\nthe remote processor [61].\\nInstead of aiming at decoding the full input signals\\nfrom the encoded representation, one can also envisage\\ndecoding only a speciﬁc signal or source that is to be\\nextracted from the input. This may enable stronger com-\\npression during encoding whenever this component hasmore restricted entropy than the full signal. In ultra-\\nsound imaging, such signal-extracting compressive deep\\nencoder–decoders can, e.g., be used for velocity estima-tion in the color Doppler [62]. Fig. 4 shows how these\\nnetworks enable decoding of the tissue Doppler signals\\nfrom the encoded IQ-demodulated input data acquired in\\nanin vivo open-chest experiment of a porcine model, using\\nintra-cardiac diverging-wave imaging in the right atrium ata frame rate of 474 Hz.\\nHere, the encoding neural network comprised a series\\nof three identical blocks, each composed of two subse-quent convolutional layers across fast- and slow time,\\nfollowed by an aggregation of this processing through\\nspatial downsampling (max pooling). The decoder had\\na similar, mirrored, architecture. The degree of IQ data\\ncompression achieved by the encoder can be changed byvarying the number of channels (in the context of image\\nprocessing often referred to as feature maps) at the latent\\nlayer. The encoder and decoder network parameters canthen be learned by mimicking the phase (and therewith,\\nvelocity) estimates obtained using the well-known Kasai\\nautocorrelator on the full input data [see Fig. 4(b)].\\nInterestingly , IQ compression rates as high as 32 can\\nbe achieved [see Fig. 4(c)] while retaining reasonableDoppler signal quality , yielding a relative phase root-mean-\\nsquared error (RMSE) of approximately 0.02. These errors\\ndrop when requiring lower compression rates. Highercompression rates lead to an increased degree of spatial\\nconsistency , displaying fewer spurious variations that could\\nnot be represented in the compact latent encoding.\\nThe design of the traditional Doppler estimators involves\\ncareful optimization of the slow- and fast-time range gates,\\nacross which the estimation is performed, amounting to\\na tradeoff between the estimation quality and spatiotem-\\nporal resolution [22]. For many practical applications,the optimal settings not only vary across measurements\\nand desired clinical objectives but also within a single mea-\\n18 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 4. (a) Tissue Doppler processing using a deep encoder–decoder network for an illustrative intracardiac ultrasound application [62],\\ndisplaying the wall between the right atrium and the aorta. (b) Deep network architecture is designed to encode input IQ data into acompressed latent space via a series of convolutional layers and spatial (max) pooling operations while maintaining the functionality andperformance of a typical Doppler processor (the Kasai autocorrelator [22]) using full uncompressed IQ data. (c) Convergence of the networkparameters during training, showing the relative RMSEs on a test data set for four data compression factors.\\nsurement. In contrast, a convolutional encoder–decoder\\nnetwork can learn to determine the effective spatiotempo-\\nral support of the given input data required for adequate\\nDoppler encoding and prediction.\\nD. Unfolding Robust PCA for Clutter Suppression\\nAn important ultrasound-based modality is CEUS [63],\\nwhich allows the detection and visualization of small bloodvessels. In particular, CEUS is used for imaging perfusion\\nat the capillary level [64], [65] and for estimating differ-\\nent vascular properties such as relative volume, velocity ,shape, and density . These physical parameters are related\\nto different clinical conditions, including cancer [66].\\nThe main idea behind CEUS is the use of encapsulated\\ngas microbubbles, serving as ultrasound contrast agents\\n(UCAs), that are injected intravenously and can ﬂowthroughout the vascular system due to their small\\nsize [67]. To visualize them, strong clutter signals\\noriginating from stationary or slowly moving tissues mustbe removed, as they introduce signiﬁcant artifacts in the\\nresulting images [68]. The latter poses a major challenge\\nin ultrasonic vascular imaging, and various methods\\nhave been proposed to address it. In [69], a high-pass\\nﬁltering approach was presented to remove tissue signalsusing ﬁnite impulse response (FIR) or inﬁnite impulse\\nresponse (IIR) ﬁlters. However, this approach is prone to\\nfailure in the presence of fast tissue motion. An alternativestrategy is the second-harmonic imaging [70] that exploits\\nthe nonlinear response of the UCAs to separate them from\\nthe tissue. This technique, however, does not remove the\\ntissue completely , as it also exhibits a nonlinear response.\\nOne of the most popular approaches for clutter suppres-\\nsion is the spatio–temporal ﬁltering based on the singular\\nvalue decomposition (SVD). This strategy has led to var-\\nious techniques for clutter removal [68], [71]–[80]. SVD\\nﬁltering includes collecting a series of consecutive frames,stacking them as vectors in a matrix, performing SVD\\nof the matrix, and removing the largest singular values,\\nassumed to be related to the tissue. Hence, a crucial step inSVD ﬁltering is determining an appropriate threshold that\\ndiscriminates between tissue- and blood-related singular\\nvalues. However, the exact setting of this threshold is\\ndifﬁcult to determine and may vary dramatically between\\ndifferent scans and subjects, leading to signiﬁcant defectsin the constructed images.\\nTo overcome these limitations, in [81]–[83], the task\\nof clutter removal was formulated as a convex optimiza-tion problem by leveraging a low-rank-and-sparse decom-\\nposition. Solomon et al. [81] then proposed an efﬁcient\\ndeep learning solution to this convex optimization problem\\nthrough an algorithm-unfolding strategy [84]. To enable\\nexplicit embedding of signal structure in the resultingnetwork architecture, the following model for the signal\\nafter beamforming was proposed.\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 19van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 5. (a) ISTA diagram for solving RPCA. (b) Diagram of a single layer of CORONA [81]. (c) Qualitative assessment of clutter removal\\nperformed by SVD ﬁltering, FISTA, and CORONA, shown in panels c1–c3, respectively. Below each panel, we present enlarged views of\\nselected areas, indicated by the green and red rectangles. (d) Quan titative assessment of clutter removal performed by the mentioned\\nmethods.\\nDenote the received beamformed signal at snapshot\\ntimetbyD(x,z,t),w h e r e (x,z)are image coordinates.\\nThen, we may write\\nD(x,z,t)=L(x,z,t)+S(x,z,t) (5)\\nwhere the term L(x,z,t)represents the tissue and S(x,z,t)\\nis the signal stemming from the blood. Similar to SVD\\nﬁltering, a series of consecutive snapshots ( t=1,...,T )\\nis acquired and stacked as vectors into a matrix, leading to\\nthe matrix model\\nD=L+S. (6)\\nThe tissue exhibits high spatio–temporal coherence; hence,\\nthe matrix Lis assumed to be low rank. The matrix Sis\\nconsidered to be sparse since small blood vessels sparsely\\npopulate the image plane.\\nThese assumptions on the rank of Land the sparsity in\\nSenable formulation of the task of clutter suppression as a\\nrobust principle component analysis (RPCA) problem [85]\\nmin\\nL,S1\\n2\\x03D−(L+S)\\x032\\nF+λ1\\x03L\\x03∗+λ2\\x03S\\x031,2 (7)\\nwhere λ1andλ2are threshold parameters. The symbol\\n\\x03·\\x03 ∗stands for the nuclear norm that sums thesingular values of L.T h et e r m \\x03·\\x03 1,2is the mixed l1,2\\nnorm [33], [86] that promotes the sparsity of the blood\\nvessels along with consistency of their locations over\\nconsecutive frames. RPCA is widely used in the area\\nof computer vision and can be solved iteratively usingthe fast iterative shrinkage/soft-thresholding algorithm\\n(FISTA) [87], leading to the following update rules\\n:\\nLk+1=STλ1/2\\n/AI1\\n2Lk−Sk+D\\n/AJ\\nSk+1=MT λ2/2\\n/AI1\\n2Sk−Lk+D\\n/AJ\\n(8)\\nwhere MT α(X)is the mixed \\x021,2soft-thresholding\\noperator that applies the function max(0 ,1−(α/\\x03x\\x03))x\\non each row xof the input matrix X. Assuming the input\\nmatrix is given by its SVD X=UΣVH, the singular value\\nthresholding (SVT) is deﬁned as STα(X)= USα(Σ) VH,\\nwhere Sα(x)=m a x ( 0 ,x−α)is applied point-wise on Σ.\\nA diagram of this iterative solution is given in Fig. 5(a).\\nAs shown in Fig. 5(c), the iterative solution (8)out-\\nperforms SVD ﬁltering and leads to improved clutter sup-\\npression. However, it suffers from two major drawbacks.\\nThe threshold parameters λ1andλ2need to be properly\\ntuned, as they have a signiﬁcant impact on the ﬁnal result.Moreover, depending on the dynamic range between the\\ntissue and the blood, FISTA may require many itera-\\ntions to converge, thus making it impractical for real-time\\n20 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nimaging. This motivates the pursuit of a solution with ﬁxed\\ncomplexity , in which the threshold parameters are adjusted\\nautomatically .\\nSuch a ﬁxed-complexity solution can be attained\\nthrough unfolding [88], [89], in which a known iterative\\nsolution is unrolled as a feedforward neural network.\\nIn this case, the iterative solution is the FISTA algo-rithm (8) that can be rewritten as\\nL\\nk+1=STλ1/2(W1D+W3Sk+W5Lk)\\nSk+1=MT λ2/2(W2D+W4Sk+W6Lk) (9)\\nwhere W1=W2=I,W3=W6=−I,a n d W4= W5=\\n(1/2)I. From this, the deep multilayer network takes a\\nform, in which the kth layer is given by\\nLk+1=STλk\\n1\\n/A0Wk\\n1∗D+Wk3∗Sk+Wk5∗Lk\\n/A1\\nSk+1=MTλk\\n2\\n/A0Wk\\n2∗D+Wk4∗Sk+Wk6∗Lk\\n/A1\\n.(10)\\nIn (10), the matrices (Wk\\n1,..., Wk6)and the regularization\\nparameters λk\\n1andλk2d i f f e rf r o mo n el a y e rt oa n o t h e r\\nand are learned during training. Moreover, (Wk\\n1,..., Wk6)\\nwere chosen to be convolution kernels, where ∗denotes\\nthe convolution operator. The latter facilitates spatial\\ninvariance along with a notable reduction in the num-ber of learned parameters. This results in a CNN that is\\nspeciﬁcally tailored for solving RPCA, whose nonlinearities\\nare the soft-thresholding and SVT operators, and is\\ntermed Convolutional rObust pRincipal cOmpoNent Analy-\\nsis (CORONA). A diagram of a single layer from CORONAis given in Fig. 5(b).\\nThe training process of CORONA is performed by back-\\npropagation in a supervised manner, leveraging both sim-ulations, for which the true decomposition is known, and\\nin vivo data, for which the decomposition of FISTA (8)is\\nconsidered as the ground truth. Moreover, data augmenta-tion is performed, and the training is done on 3-D patches\\nextracted from the input measurements. The loss function\\nwas chosen as the sum of mean squared errors (mse)\\nE\\n(θ)=1\\n2N\\n/AWN/CG\\ni=1\\x03Si−ˆSi(θ)\\x032\\nF+\\x03Li−ˆLi(θ)\\x032\\nF\\n/AX\\n(11)\\nwhere {Si,Li}N\\ni=1are the ground truth and {ˆSi,ˆLi}N\\ni=1are\\nthe network’s outputs. The learned parameters are denoted\\nbyθ={Wk\\n1,..., Wk6,λk1,λk2}K\\nk=1,w h e r e Kis the number of\\nlayers. Backpropagation through the SVD was done usingPyTorch’s Autograd function [90].\\nFig. 5 shows how CORONA effectively suppresses clut-\\nter on CEUS scans of two rat’s brains, outperform-\\ning SVD ﬁltering and RPCA through FISTA (8).T h e\\nrecovered CEUS (blood) signals are given in Fig. 5(c),including the enlarged views of regions of inter-\\nest. Visually judging, FISTA achieves moderately bettercontrast than SVD ﬁltering, while CORONA outperforms\\nboth approaches by a large margin. For a quantitative\\ncomparison, the CNR and the contrast ratio (CR) were\\nassessed, deﬁned as\\nCNR\\n=|μs−μb|/D4\\nσ2s+σ2\\nbCR=μs\\nμb(12)\\nwhere μsandσ2\\nsare the mean and variance of the\\nregions of interest in Fig. 5(c), and μbandσ2\\nbare the\\nmean and variance of the noisy reference area indicated\\nby the yellow box. In both metrics, higher values imply\\nhigher CRs, which suggests better noise suppression. FISTA\\nobtained slightly better performance than SVD ﬁltering\\n(CR≈4.6 dB and ≈5.4 dB, respectively), and CORONA\\noutperformed both (CR ≈15 dB). In most cases, the per-\\nformance of CORONA was about an order of magni-\\ntude better than that of SVD. Thus, combining a modelfor the separation problem wi th a data-driven approach\\nleads to improved separation of UCA and tissue signals,\\ntogether with noise reduction compared to the popular\\nSVD approach.\\nThe complexity of all three methods is governed by\\nthe singular-value decomposition that requires\\nO(MN2)\\nFLOPS for an M×Nmatrix, where M≥N. However,\\nFISTA may require thousands of iterations, i.e., thousandsof such SVD operations. Hence, FISTA for RPCA is compu-\\ntationally signiﬁcantly heavier than regular SVD-ﬁltering.\\nOn the other hand, for CORONA, up to ten layers were\\nshown to be sufﬁcient (i.e., up to ten SVD operations),\\ntherewith offering a dramatic increase in performance atthe expense of only a moderate increase in complexity . All\\nthree methods can beneﬁt from using inexact decomposi-\\ntions that exhibit reduced computational load, such as thetruncated SVD and randomized SVD.\\nIV. DEEP LEARNING FOR\\nSUPER-RESOLUTION\\nA. Ultrasound Localization Microscopy\\nWhile the above-described advances in front-end ultra-\\nsound processing can boost resolution, suppress clutter,\\nand drastically improve tissue contrast, the attainable res-\\nolution of ultrasonography remains fundamentally limitedby wave diffraction, i.e., the minimum distance between\\nseparable scatters is half a wavelength. Simply increas-\\ning the transmit frequency to shorten the wavelength,unfortunately , comes at the cost of reduced penetration\\ndepth since higher frequencies suffer from stronger absorp-\\ntion compared to waves with a higher wavelength. Thistradeoff between resolution and penetration depth particu-\\nlarly hampers deep high-resolution microvascular imaging,\\nbeing a cornerstone for many diagnostic applications.\\nRecently , this tradeoff was circumvented by the intro-\\nduction of ULM [91], [92]. ULM leverages principles thatformed the basis for the Nobel-prize-winning concept\\nfrom optics of super-resolution ﬂuorescence microscopy\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 21van Sloun et al. : Deep Learning in Ultrasound Imaging\\nand adapts these to ultrasound imaging: if individual\\npoint sources are well isolated from diffraction-limited\\nscans, and their centers subsequently precisely pinpointed\\non a subdiffraction grid, then the accumulation of manysuch localizations over time yields a super-resolved image.\\nIn optics, stochastic “blinking” of subsets of ﬂuorophores is\\nexploited to provide such sparse point sources. In ULM,intravascular lipid-shelled gas microbubbles fulﬁll this\\nrole [93]. This approach permits achieving a resolution\\nthat is up to ten times smaller than the wavelength [8].\\nSince the ﬁdelity of ULM depends on the number\\nof localized microbubbles and the localization accuracy ,it gives rise to a new tradeoff that balances the required\\nmicrobubble sparsity for accurate localization and acqui-\\nsition time. To achieve the desired signal sparsity forstraightforward isolation of the backscattered echoes, ULM\\nis typically performed using a very diluted solution of\\nmicrobubbles. On regular ultrasound systems, this con-\\nstraint leads to tediously long acquisition times (on the\\norder of hours) to cover the full vascular bed. Usingan ultrafast plane-wave ultrasound system rather than\\nregular scanning, Errico et al. [8] performed ultrafast\\nULM (uULM) in a rat’s brain. Empowered by high framerates (500 frames/s), the acquisition time was lowered to\\nminutes instead of hours. Ultrafast imaging indeed enables\\ntaking many snapshots of individual microubbles, as they\\ntransport through the vasculature, thereby facilitating very\\nhigh-ﬁdelity reconstruction of the larger vessels. Never-theless, mapping the full capillary bed remains dictated\\nby the requirement of microbubbles to pass through each\\nof the capillaries. As such, long acquisitions of tens ofminutes are required, even with uULM [94]. To boost\\nthe achieved coverage in a given time-span, methods\\nthat enable the use of higher concentrations can be\\nleveraged [32], [33], [95]–[97].\\nB. Exploiting Signal Structure\\nTo strongly relax the constraints on microbubble con-\\ncentration and therewith cover more vessels in a shorter\\ntime, standard ULM can be extended by incorporating\\nknowledge of the measured signal structure, in particular,\\nits sparsity in a transform domain. To that end, a receivedcontrast-enhanced image frame can be modeled as\\ny\\n=Ax+w (13)\\nwhere xis a vector that describes the sparse microbub-\\nble distribution on a high-resolution image grid, yis the\\nvectorized image frame of the ultrasound sequence, Ais\\nthe measurement matrix, where each column of Ais\\nthe point-spread-function shifted by a single pixel on the\\nhigh-resolution grid, and wis a noise vector.\\nLeveraging this signal prior, i.e., assuming that the\\nmicrobubble distribution is sparse on a sufﬁcientlyhigh-resolution grid (or, the number of nonzero entries in\\nxis low), we can formulate the following\\n\\x021-regularizedinverse problem :\\nˆx=a r gm i n\\nx\\x03y−Ax\\x032\\n2+λ\\x03x\\x031 (14)\\nwhere λis a regularization parameter that weighs the\\ninﬂuence of \\x03x\\x031.\\nEquation (14) may be solved using a numerical proximal\\ngradient scheme, such as FISTA [87]. We will discuss this\\nFISTA-based solution in Section IV-C2. After estimating x\\nfor each frame, the estimates are summed across all frames\\nto yield the ﬁnal super-resolution image.\\nBeyond sparsity on a frame-by-frame basis, signal struc-\\nture may also be leveraged across multiple frames. To that\\nend, a multiple-measurement vector model [98] and its\\nstructure in a transformed domain can be considered, e.g.,\\nby assuming that a temporal stack of frames xis sparse in\\nthe temporal correlation domain [32], [33]. Consideringthe temporal dimension, sparse recovery may be improved\\nby exploiting the motion of microbubbles, allowing the\\napplication of a prior on the spatial microbubble distrib-ution through the Kalman tracking [99].\\nExploiting signal structure through sparse recovery\\nindeed enables improved localization precision and recall\\nfor high microbubble concentrations [95], [97]. Unfortu-\\nnately , proximal gradient schemes, such as FISTA, typi-cally require numerous iterations to converge (yielding\\na very time-consuming reconstruction process), and their\\neffectiveness is strongly dependent on careful tuning ofthe optimization parameters (e.g.,\\nλand the step size).\\nIn addition, the linear model in (13) is an approxima-\\ntion of what is actually a nonlinear relation between the\\nmicrobubble distribution and the resulting beamformed\\nand envelope-detected image frame. While this approxi-mation is valid for microbubbles that are sufﬁciently far\\napart, the signiﬁcant image-domain implications of the RF\\ninterference patterns of very closely spaced microbubblescannot be neglected.\\nC. Deep Learning for Fast High-Fidelity Sparse\\nRecovery\\n1) Encoder–Decoder Architectures: In pursuit of fast\\nand robust sparse recovery for the nonlinear measure-\\nment model, we leveraged deep learning to solve thecomplex inverse problem based on adequate simula-\\ntions of the forward problem [95], [96]. This data-driven\\napproach, named deep-ULM, harnesses a fully convolu-tional neural network to map a low-resolution input image\\ncontaining many overlapping microbubble signals, to a\\nhigh-resolution sparse output image, in which the pixelintensities reﬂect recovered backscatter levels. This process\\nis illustrated in Fig. 6(a). The network comprises an\\nencoder and a decoder, with the former expressing input\\nframes in a latent representation and the latter decoding\\nsuch representation into a high-resolution output. Theencoder is composed of a contracting path of three blocks,\\neach block consisting of two successive\\n3×3convolution\\n22 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 6. (a) Fast ULM through deep learning (deep-ULM) [95], [96], using a convolutional neural network to map low-resolution CEUS frames\\nto highly resolved sparse localizations on an eight times ﬁner grid. The network is trained using realistic simulations of the correspondingultrasound acquisitions, incorporating a point-spread-function estimate, the modulation frequency, pixel spacing, and background noise assampled from real data sets. (b) Standard maximum intensity projection across a sequence of frames for a rat’s spinal cord.(c) Corresponding deep-ULM reconstruction.\\nlayers and one 2×2max-pooling operation. This is fol-\\nlowed by two 3×3convolutional layers and a dropout layer\\nthat randomly disables nodes with a probability of 0.5 to\\nmitigate overﬁtting. The subsequent decoder also consists\\nof three blocks; the ﬁrst two blocks encompassing two\\n5×5convolution layers, of which the second has an\\noutput stride of 2, followed by 2×2nearest-neighbor\\nup-sampling. The last block consists of two convolution\\nlayers, of which the second again has an output stride of 2,preceding another\\n5×5convolution that maps the feature\\nspace to a single-channel image through a linear activation\\nfunction. All other activation functions in the network\\nwere leaky ReLUs [100]. The full deep encoder–decoder\\nnetwork [see Fig. 6(a)] effectively scales the input imagedimensions up by a factor\\n8and provides a powerful\\nmodel that has the capacity to learn the sparse decoding\\nproblem while yielding simultaneous denoising throughthe compact latent space.\\nThe network is trained on simulations of CEUS acqui-\\nsitions, using an estimate of the real system point-spread-\\nfunction, the RF modulation frequency , and pixel spacing.\\nNoise, clutter, and artifacts were included by randomlysampling from real measurements across frames, in which\\nno microbubbles are present. Similar to [101], we adopt a\\nspeciﬁc loss function that acts as a surrogate for the reallocalization error\\nL(Y,Xt|θ)=\\x03f(Y|θ)−G(σ)∗Xt\\x032\\n2+γ\\x03f(Y|θ)\\x031(15)where Yand Xtare the low-resolution input and sparse\\nsuper-resolution target frames, respectively , f(Y|θ)is the\\nnonlinear neural network function, and G(σ)is an isotropic\\nGaussian convolution kernel. Jointly , the \\x021penalty that\\nacts on the reconstructions and the kernel G(σ)that oper-\\nates on the targets yield a loss function that increaseswhen the reconstructed images exhibit less sparsity and\\nwhen the Euclidean distances between the localizations\\nand the targets become larger. We note that the selectionof the relative weighting of this sparsity penalty by\\nγis\\nless critical than the thresholding parameter λadopted\\nin the sparse recovery problem (14) since the measure-\\nment model A(characterized by the point-spread-function)\\nexhibits a much smaller bandwidth than G(σ)for low\\nvalues of σas adopted here. Consequently , the degree of\\nbandwidth extension necessary to yield sparse outputs is\\nless in the latter case.\\nFig. 6(c) displays the super-resolution ultrasound\\nreconstruction of a rat’s spinal cord [102], qualitatively\\nshowing how deep-ULM achieves a signiﬁcantly higher\\nresolution and contrast than the diffraction-limited\\nmaximum intensity projection image [see Fig. 6(b)].Deep-ULM achieves a resolution of about 20–30\\nμm, being\\na 4–5-fold improvement compared to standard imaging\\nwith the adopted linear 15-MHz transducer [95]. In termsof speed, recovery on a 4096\\n×1328 grid takes roughly\\n100 ms/frame using GPU acceleration, making it about\\nfour orders of magnitude faster than a Fourier-domain\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 23van Sloun et al. : Deep Learning in Ultrasound Imaging\\nFig. 7. (a) Deep encoder–decoder architecture used in Deep-ULM [95], [96]. (b) Deep unfolded ULM architecture obtained by unfolding the\\nISTA scheme, as shown in Section IV-C2. (c) Performance comparison of standard ULM, sparse-recovery, deep-ULM, and deep unfolded ULMon simulations. (d) Deep unfolded ULM for super-resolution vascular imaging of a rat’s spinal cord. Both deep learning approachesoutperform the other methods. While Deep-ULM shows a higher recall and slightly lower localization error as compared to deep unfoldedULM on simulation data, the latter seems to generalize better toward in vivo acquisitions, qualitatively yielding images with higher ﬁdelity[see Fig. 6(c) for comparison].\\nimplementation of sparse recovery through the FISTA\\nproximal gradient scheme [33].\\n2) Deep Unfolding for Robust and Fast Sparse Decoding:\\nWhile deep encoder–decoder architectures (as used indeep-ULM) serve as a general model for many regression\\nproblems and are widely used in computer vision, their\\nlarge ﬂexibility and capacity also likely make them over-\\nparameterized for the sparse decoding problem at hand.To promote robustness by exploiting knowledge of the\\nunderlying signal structure (i.e., microbubble sparsity),\\nwe propose using a dedicated and more compact networkarchitecture that borrows inspiration from the proximal\\ngradient methods introduced in Section IV-B [87].\\nTo do so, we ﬁrst brieﬂy describe the ISTA scheme for\\nthe sparse decoding problem in (14)\\nx\\nk+1=Tλ(xk−μAT(Axk−y)) (16)\\nwhere μdetermines the step size, and Tλ(x)i=(|xi|−\\nλ)+sgn(xi)is the proximal operator of the \\x021norm.\\nEquation (16) is compactly written as\\nxk+1=Tλ(W1y+W2xk) (17)with W1=μATand W2= I−μATA. Similar to our\\napproach to robust PCA in Section III-D, we can unfold\\nthis recurrent structure into a K-layer feedforward neural\\nnetwork, as in LISTA (“learning ISTA ”) [88], with each\\nlayer consisting of trainable convolutions Wk\\n1and Wk\\n2,\\nalong with a trainable shrinkage parameter λk.T h i s\\nenables learning a highly efﬁcient ﬁxed-length iterative\\nscheme for fast and robust ULM, with an optimal setof kernels and parameters per iteration, which we term\\ndeep unfolded ULM. Different than LISTA, we avoid\\nvanishing gradients in the “dead zone” of the proximalsoft-thresholding operator\\nTλ, by replacing it by a\\nsmooth sigmoid-based soft-thresholding operation [103].\\nAn overview of this approach is given in Fig. 7(b),\\ncontrasting this dedicated sparse-decoding-inspired\\nsolution with a general deep encoder–decoder networkarchitecture in Fig. 7(a). Both networks are trained on the\\nsame, synthetically generated, data.\\nTests on synthetic data show that both deep learn-\\ning methods signiﬁcantly outperform standard ULM and\\nsparse decoding through FISTA for high microbubble con-\\ncentrations [see Fig. 7(c)]. On such simulations, the deep\\nencoder–decoder used in deep-ULM yields higher recall\\nand lower localization errors compared to the deep\\n24 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nunfolded ULM. Interestingly , when applying the trained\\nnetworks to in vivo ultrasound data, we instead observe\\nthat deep unfolded ULM yields super-resolution images\\nwith higher ﬁdelity . Thus, it is capable of translatingmuch better toward real acquisitions than the large deep\\nencoder–decoder network [see Figs. 6(c) and 7(d) for\\ncomparison].\\nOur ten-layer deep unfolded ULM comprising 5\\n×5\\nconvolutional kernels has much fewer parameters (merely\\n506, compared to almost 700 000 for the encoder–decoder\\nscheme), therefore exhibiting a drastically lower memory\\nfootprint and reduced power consumption, in additionto achieving higher inference rates. The encoder–decoder\\napproach requires over four million FLOPS for map-\\nping a low-resolution patch of 16\\n×16 pixels into a\\nsuper-resolution patch of 128 ×128 pixels. The unfolded\\nISTA architecture is much more efﬁcient, requiring just\\nover 1000 FLOPS.\\nThe lower number of trainable parameters may also\\nexplain the improved robustness and better generalizationtoward real data compared to its over-parameterized coun-\\nterpart. On the other hand, complex image artifacts, such\\nas the strong bone reﬂections visible in the bottom leftof Fig. 7(d), remain more prominent using the compact\\nunfolding scheme.\\nV. OTHER APPLICA TIONS OF DEEP\\nLEARNING IN ULTRASOUND\\nWhile this article predominantly focuses on deep learn-\\ning strategies for ultrasound-speciﬁc receive processingmethods along the imaging chain, the initially most\\nthriving application of deep learning in ultrasound was\\nspurred by computer vision: automated analysis of theimages obtained with traditional systems [104]. Such\\nimage analysis methods aim at dramatically accelerating\\n(and potentially improving) current clinical diagnostics.\\nA classic application of ultrasonography lies in\\nprenatal screening, where fetal growth and developmentare monitored to identify possible problems and aid\\ndiagnosis. These routine examinations can be complex\\nand cumbersome, requiring years of training to swiftlyidentify the scan planes and structures of interest.\\nBaumgartner et al. [105] effectively leverage deep\\nlearning to drastically simplify this procedure, enabling\\nreal-time detection and localization of standard fetal\\nscan planes in freehand ultrasound. Similarly , in [106]and [107], deep learning was used to accelerate\\nechocardiographic exams by automatically recognizing the\\nrelevant standard views for further analysis, even permit-ting automated myocardial strain imaging [108]. In [109],\\nCNN was trained to perform thyroid nodule detection and\\nrecognition. Similar applications of deep learning include\\nautomated identiﬁcation and segmentation of tumors in\\nbreast ultrasound [110]–[112], localization of clinicallyrelevant B-line artifacts in lung ultrasonography [113],\\nand real-time segmentation of anatomical zones ontransrectal ultrasound (TRUS) scans [114]. Hu et al. [115]\\nshow how such anatomical landmarks and boundaries can\\nbe exploited by a deep neural network to attain accurate\\nvoxel-level registration of TRUS and MRI.\\nBeyond these computer-vision applications, other\\nlearning-based techniques aim at extracting relevant\\nmedium parameters for tissue characterization. Amongsuch approaches is data-driven elasticity imaging [116],\\n[117]. In these works, the authors propose neural-\\nnetwork-based models that produce spatially varying lin-\\near elastic material properties from force–displacement\\nmeasurements, free from prior assumptions on the under-lying constitutive models or material properties. In [118],\\na deep convolutional neural network is used for speed-of-\\nsound estimation from the (single-sided) B-mode channeldata. Vishnevskiy et al. [119] address the problem by\\nintroducing an unfolding strategy to yield a dedicated\\nnetwork based on the iterative wave reﬂection tracking\\nalgorithm. The ability to measure the speed of sound\\nnot only permits tissue characterization but also adequaterefraction-correction in beamforming.\\nVI. DISCUSSION AND FUTURE\\nPERSPECTIVES\\nOver the past years, deep learning has revolutionized\\na number of domains, spurring breakthroughs in com-\\nputer vision, natural language processing, and beyond.\\nIn this article, we aimed to signify the potential that this\\npowerful approach carries when leveraged in ultrasound\\nimage and signal reconstruction. We argue and show thatdeep learning methods proﬁt considerably when integrat-\\ning signal priors and structure, embodied by the pro-\\nposed deep unfolding schemes for clutter suppression andsuper-resolution imaging, and the learned beamforming\\napproaches. In addition, several ultrasound-speciﬁc con-\\nsiderations regarding suitable activation and loss functions\\nwere given.\\nWe designed and showcased a number of independent\\nbuilding blocks, with trained artiﬁcial agents and neural\\nsignal processors dedicated to distinct applications.\\nSome of the presented methods operate on images(see Sections III-D and IV) or IQ data (see Section III-C),\\nwhile others process channel data directly (see\\nSections III-A and III-B). A full processing chain may\\neasily comprise a number of such components, which can\\nbe optimized holistically . This proposition enables imagingchains that are dedicated to the application and fully\\nadaptive.\\nDesigning neural networks that can efﬁciently process\\nchannel data in real time comes with a number of chal-\\nlenges. First, in contrast to images, channel data has a\\nvery large dynamic range and is RF modulated. This\\nmakes typical activation functions as used in image analy-\\nsis (often ReLUs or hyperbolic tangents) less suited.In Section III-A3, we argue that the class of concatenated\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 25van Sloun et al. : Deep Learning in Ultrasound Imaging\\nReLUs provides a possible alternative. Second, channel\\ndata is extremely large, in particular, for large arrays\\nor matrix transducers and when sampled at the Nyquist\\nrate. This may be alleviated signiﬁcantly by leveragingsub-Nyquist sampling schemes [3], [14], [15], [17], [55],\\npermitting high-end processing of low-rate channel data\\nafter (wireless) transfer to a remote (or cloud) processor.Such a new scheme, with a wireless probe that streams\\nlow-rate channel data for subsequent deep learning in\\nthe cloud, would open up many new possibilities for\\nintelligent image formation and advanced processing in\\nultrasonography.\\nDeep learning typically relies on vast amounts of train-\\ning data. Although several approaches to make learn-\\ning more data-efﬁcient and robust have been discussedthroughout this article, a signiﬁcant amount of data is still\\nrequired. In the framework of supervised learning, training\\ndata typically consist of input data and desired targets.\\nWhat these targets are and how they should be obtained\\ndepends on the application and goal. Sometimes, it is, forinstance, desirable to mimic an existing high-performance\\nalgorithm that is too complex and costly to implement\\nin real time. Examples of this are the adaptive beam-forming and spectral Doppler applications described in\\nSections III-A and III-B, respectively . At other times, train-\\ning data may only be obtainable through simulations or\\nmeasurements on well-characterized in vitro phantoms.\\nIn such cases, the performance of a deep learning algo-rithm on in vivo data stands or falls with the realism\\nof these training data and its coverage of the real-world\\ndata distribution. As shown in Section IV-C2, leveragingstructural signal priors in the network architecture strongly\\naids generalization beyond simulations.\\nOnce trained, inference can be fast through the exploita-\\ntion of high-performance GPUs. While advanced high-end\\nimaging systems may be equipped with GPUs to facilitatethe deployment of deep neural networks at the remote\\nprocessor, FPGAs or ASICSs may be more appropriate\\nfor resource-limited low-power settings [120]. In the con-sumer market, small neural- and tensor-processing units\\n(NPUs and TPUs, respectively) are enabling neural net-\\nwork inference at the edge [121]—one can envisage a\\nsimilar paradigm for the front-end ultrasound process-\\ning. As such, the relevance of designing compact andefﬁcient neural networks for memory-constrained (edge)\\nsettings is considerable and becomes particularly relevantfor miniature and highly portable ultrasound systems,\\nwhere memory size, inference speed, and network band-\\nwidth are all strictly constrained. This may be achieved\\nby favoring (multiple) artiﬁcial agents that have very spe-ciﬁc and well-deﬁned tasks (see Sections III-A and III-B),\\nas opposed to a single highly complex end-to-end deep\\nneural network. We also showed that embedding signalpriors in neural architectures permits drastically reduced\\nmemory footprints. In that context, the difference between\\na deep convolutional encoder–decoder network (no prior)\\nand a deep unfolded ISTA network (structural spar-\\nsity prior) is illustrative, where the former consists ofalmost 700 000 parameters and the latter can perform\\nsuper-resolution recovery with just over 500. Additional\\nstrategies to condense large models include knowledgedistillation [122] and parameter pruning, as well as weight\\nquantization [123].\\nOnce deployed in the ﬁeld, artiﬁcial agents in\\nnext-generation ultrasound systems ultimately should be\\nable to embrace the vastness of data at their disposal tocontinuously learn throughout their “lifetime.” To that end,\\nunsupervised or self-supervised learning become increas-\\ningly relevant [124]. This holds true for many artiﬁcialintelligence applications and extends beyond ultrasound\\nimaging.\\nThe promise that deep learning holds for ultrasound\\nimaging is signiﬁcant; it may spur a paradigm shift in\\nthe design of ultrasound systems, where smart wirelessprobes facilitated by sub-Nyquist and neural edge comput-\\ning are connected to the cloud and with AI-driven imaging\\nmodes and algorithms that are dedicated to speciﬁc appli-cations. Empowered by deep learning, next-generation\\nultrasound imaging may become a much stronger modality\\nwith devices that continuously learn to provide better\\nimages and clinical insight, leading to improved and more\\nwidely accessible diagnostics through cost-effective, highlyportable, and intelligent imaging.\\nAcknowledgments\\nThe authors would like to thank B. Luijten, F . de Bruijn andH. Schmeitz for their contribution to the adaptive beam-\\nforming and spectral Doppler applications. They would\\nalso like to thank M. Bruce and Z. Khaing for acquiringthe spinal cord data used to evaluate the super-resolution\\nalgorithms.\\nREFERENCES\\n[1] T . L. Szabo, Diagnostic Ultrasound Imaging: Inside\\nOut. New York, NY , USA: Academic, 2004.\\n[2] J. M. Baran and J. G. Webster, “Design of low-cost\\nportable ultrasound systems: Review ,” in Proc.\\nAnnu. Int. Conf. IEEE Eng. Med. Biol. Soc. ,\\nSep. 2009, pp. 792–795.\\n[3] T . Chernyakova and Y. Eldar, “Fourier-domain\\nbeamforming: The path to compressed ultrasound\\nimaging,” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 61, no. 8, pp. 1252–1267, Aug. 2014.\\n[4] J. Provost et al. , “3D ultrafast ultrasound imaging\\nin vivo ,”P h y s .M e d .B i o l . , vol. 59, no. 19, p. L1,\\n2014.[5] M. Tanter and M. Fink, “Ultrafast imaging in\\nbiomedical ultrasound,” IEEE Trans. Ultrason.,\\nFerroelectr ., Freq. Control ,v o l .6 1 ,n o .1 ,\\npp. 102–119, Jan. 2014.\\n[6] J. Bercoff, M. Tanter, and M. Fink, “Supersonic\\nshear imaging: A new technique for soft tissueelasticity mapping,” IEEE Trans. Ultrason.,\\nFerroelectr ., Freq. Control ,v o l .5 1 ,n o .4 ,\\npp. 396–409, Apr . 2004.\\n[7] C. Demené et al. , “Spatiotemporal clutter ﬁltering\\nof ultrafast ultrasound data highly increasesDoppler and fultrasound sensitivity ,” IEEE Trans.\\nMed. Imag. , vol. 34, no. 11, pp. 2271–2285,Nov. 2015.\\n[8] C. Errico et al. , “Ultrafast ultrasound localization\\nmicroscopy for deep super-resolution vascular\\nimaging,” Nature , vol. 527, no. 7579, p. 499,\\n2015.\\n[ 9 ] A .B e s s o n ,D .P e r d i o s ,M .A r d i t i ,Y .W i a u x ,a n d\\nJ.-P. Thiran, “Compressive multiplexing of\\nultrasound signals,” in Proc. IEEE Int. Ultrason.\\nSymp. (IUS) , Oct. 2018, pp. 1–4.\\n[10] Y . C. Eldar , Sampling Theory: Beyond Bandlimited\\nSystems . Cambridge, U.K.: Cambridge Univ. Press,\\n2015.\\n[11] K. Gedalyahu, R. Tur, and Y. C. Eldar,\\n26 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\n“Multichannel sampling of pulse streams at the\\nrate of innovation,” IEEE Trans. Signal Process. ,\\nvol. 59, no. 4, pp. 1491–1504, Apr . 2011.\\n[12] R. Tur, Y. C. Eldar, and Z. Friedman, “Innovation\\nrate sampling of pulse streams with application to\\nultrasound imaging,” IEEE Trans. Signal Process. ,\\nvol. 59, no. 4, pp. 1827–1842, Apr . 2011.\\n[13] Y. C. Eldar and G. Kutyniok, Compressed Sensing:\\nTheory and Applications . Cambridge, U.K.:\\nCambridge Univ. Press, 2012.\\n[ 1 4 ] N .W a g n e r ,Y .C .E l d a r ,A .F e u e r ,G .D a n i n ,a n d\\nZ. Friedman, “Xampling in ultrasound imaging,”\\nProc. SPIE , vol. 7968, p. 796818, Mar . 2011.\\n[ 1 5 ] N .W a g n e r ,Y .C .E l d a r ,a n dZ .F r i e d m a n ,\\n“Compressed beamforming in ultrasoundimaging,” IEEE Trans. Signal Process. , vol. 60,\\nno. 9, pp. 4643–4657, Sep. 2012.\\n[16] M. Mishali, Y . C. Eldar, and A. J. Elron, “Xampling:\\nSignal acquisition and processing in union of\\nsubspaces,” IEEE Trans. Signal Process. , vol. 59,\\nno. 10, pp. 4719–4734, Oct. 2011.\\n[17] M. Mishali, Y. C. Eldar, O. Dounaevsky , and\\nE. Shoshan, “Xampling: Analog to digital at\\nsub-Nyquist rates,” IET Circuits, Devices Syst. ,\\nvol. 5, no. 1, pp. 8–20, Jan. 2011.\\n[18] T . Michaeli and Y. C. Eldar, “Xampling at the rate\\nof innovation,” IEEE Trans. Signal Process. , vol. 60,\\nno. 3, pp. 1121–1133, 2012.\\n[19] T . Chernyakova et al. , “Fourier-domain\\nbeamforming and structure-based reconstruction\\nfor plane-wave imaging,” IEEE Trans. Ultrason.,\\nFerroelectr ., Freq. Control , vol. 65, no. 10,\\npp. 1810–1821, Oct. 2018.\\n[20] A. Burshtein, M. Birk, T . Chernyakova, A. Eilam,\\nA. Kempinski, and Y . C. Eldar , “Sub-Nyquist\\nsampling and Fourier domain beamforming in\\nvolumetric ultrasound imaging,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control ,v o l .6 3 ,n o .5 ,\\npp. 703–716, May 2016.\\n[21] A. Lahav, T . Chernyakova, and Y. C. Eldar, “FoCUS:\\nFourier-based coded ultrasound,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control ,v o l .6 4 ,\\nno. 12, pp. 1828–1839, Dec. 2017.\\n[ 2 2 ] T .L o u p a s ,J .T .P o w e r s ,a n dR .W .G i l l ,“ A na x i a l\\nvelocity estimator for ultrasound blood ﬂow\\nimaging, based on a full evaluation of the Dopplerequation by means of a two-dimensional\\nautocorrelation approach,” IEEE Trans. Ultrason.,\\nFerroelectr ., Freq. Control ,v o l .4 2 ,n o .4 ,\\npp. 672–688, Jul. 1995.\\n[23] P . D. Welch, “The use of fast Fourier transform for\\nthe estimation of power spectra: A method based\\non time averaging over short, modiﬁed\\nperiodograms,” IEEE Trans. Audio Electroacoust. ,\\nvol. AE-15, no. 2, pp. 70–73, Jun. 1967.\\n[24] K. Nightingale, “ Acoustic radiation force\\nimpulse (ARFI) imaging: A review ,” Current. Med.\\nImag. Rev. , vol. 7, no. 4, pp. 328–339, Nov. 2011.\\n[ 2 5 ] R .J .G .v a nS l o u n ,R .R .W i l d e b o e r ,H .W i j k s t r a ,\\nand M. Mischi, “Viscoelasticity mapping by\\nidentiﬁcation of local shear wave dynamics,” IEEE\\nTrans. Ultrason., Ferroelectr ., Freq. Control , vol. 64,\\nno. 11, pp. 1666–1673, Nov. 2017.\\n[26] B. B. Goldberg, J.-B. Liu, and F . Forsberg,\\n“Ultrasound contrast agents: A review ,”\\nUltrasound Med. Biol. , vol. 20, no. 4, pp. 319–333,\\n1994.\\n[ 2 7 ] R .J .G .v a nS l o u n ,L .D e m i ,A .W .P o s t e m a ,\\nJ. J. de la Rosette, H. Wijkstra, and M. Mischi,\\n“Ultrasound-contrast-agent dispersion and\\nvelocity imaging for prostate cancer localization,”\\nMed. Image Anal. , vol. 35, pp. 610–619, Jan. 2017.\\n[28] M. F . Hamilton et al. Nonlinear Acoustics , vol. 237.\\nSan Diego, CA, USA: Academic, 1998.\\n[29] Y . Desailly , A.-M. Tissier , J.-M. Correas,\\nF . Wintzenrieth, M. Tanter , and O. Couture,\\n“Contrast enhanced ultrasound by real-timespatiotemporal ﬁltering of ultrafast images,” Phys.\\nMed. Biol. , vol. 62, no. 1, p. 31, 2016.\\n[30] O. M. Viessmann, R. J. Eckersley ,\\nK. Christensen-Jeffries, M. X. Tang, and C. Dunsby ,“Acoustic super-resolution with ultrasound and\\nmicrobubbles,” P h y s .M e d .B i o l . , vol. 58, no. 18,\\np. 6447, Sep. 2013.\\n[31] M. A. O’Reilly and K. Hynynen, “A super-resolutionultrasound method for brain vascular mapping,”\\nMed. Phys. , vol. 40, no. 11, 2013, Art. no. 110701.\\n[32] A. Bar-Zion, C. Tremblay-Darveau, O. Solomon,\\nD. Adam, and Y. C. Eldar, “Fast vascular\\nultrasound imaging with enhanced spatial\\nresolution and background rejection,” IEEE Trans.\\nMed. Imag. , vol. 36, no. 1, pp. 169–180,\\nJan. 2017.\\n[33] A. Bar-Zion, O. Solomon, C. Tremblay-Darveau,\\nD. Adam, and Y . C. Eldar , “SUSHI: Sparsity-based\\nultrasound super-resolution hemodynamicimaging,” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 65, no. 12, pp. 2365–2380,\\nDec. 2018.\\n[34] K. Hornik, M. Stinchcombe, and H. White,\\n“Multilayer feedforward networks are universal\\napproximators,” Neural Netw. ,v o l .2 ,n o .5 ,\\npp. 359–366, 1989.\\n[35] V . Mnih et al. , “Human-level control through deep\\nreinforcement learning,” Nature , vol. 518,\\nno. 7540, p. 529, 2015.\\n[36] I. Goodfellow , Y . Bengio, and A. Courville, Deep\\nlearning . Cambridge, MA, USA: MIT Press, 2016.\\n[37] R. Mallart and M. Fink, “Sound speed ﬂuctuations\\nin medical ultrasound imaging comparison\\nbetween different correction algorithms,” in\\nAcoustical Imaging (Acoustical Imaging Book\\nSeries), vol. 19. Springer , 1992, pp. 213–218.\\n[38] H. L. Van Trees, Optimum Array Processing: Part IV\\nof Detection, Estimation, and Modulation Theory .\\nHoboken, NJ, USA: Wiley , 2004.\\n[ 3 9 ] S .V e d u l a ,O .S e n o u f ,G .Z u r a k h o v ,A .B r o n s t e i n ,\\nO. Michailovich, and M. Zibulevsky , “Learning\\nbeamforming in ultrasound imaging,” 2018,\\narXiv:1812.08043 . [Online]. Available:\\nhttps://arxiv.org/abs/1812.08043\\n[40] D. Perdios, A. Besson, M. Arditi, and J.-P .T h i r a n ,\\n“A deep learning approa ch to ultrasound image\\nrecovery ,” in Proc. IEEE Int. Ultrason. Symp. (IUS) ,\\nSep. 2017, pp. 1–4.\\n[41] W . Simson et al. , “End-to-end learning-based\\nultrasound reconstruction,” 2019,\\narXiv:1904.04696 . [Online]. Available:\\nhttps://arxiv.org/abs/1904.04696\\n[42] S. Khan, J. Huh, and J. C. Ye, “Universal deep\\nbeamformer for variable rate ultrasoundimaging,” 2019, arXiv:1901.01706 .[ O n l i n e ] .\\nAvailable: https://arxiv.org/abs/1901.01706\\n[43] A. C. Luchies and B. C. Byram, “Deep neural\\nnetworks for ultrasound beamforming,” IEEE\\nTrans. Med. Imag. , vol. 37, no. 9, pp. 2010–2021,\\nSep. 2018.\\n[44] D. Hyun, L. L. Brickson, K. T . Looby , and J. J. Dahl,\\n“Beamforming and speckle reduction using neuralnetworks,” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 66, no. 5, pp. 898–910, May 2019.\\n[45] P .C o u p é ,P .H e l l i e r ,C .K e r v r a n n ,a n dC .B a r i l l o t ,\\n“Nonlocal means-based speckle ﬁltering forultrasound images,” IEEE Trans. Image Process. ,\\nvol. 18, no. 10, pp. 2221–2229, Oct. 2009.\\n[46] O. Senouf et al. , “High frame-rate cardiac\\nultrasound imaging with deep learning,” in Proc.\\nInt. Conf. Med. Image Comput.-Assist. Intervent\\n(MICCAI) . Granada, Spain: Springer , 2018,\\npp. 126–134.\\n[47] S. Goudarzi, A. Asif, and H. Rivaz, “Multi-focus\\nultrasound imaging using generative adversarial\\nnetworks,” in P r o c .I E E EI n t .S y m p .B i o m e d .I m a g .\\n(ISBI) , Apr . 2019, pp. 1118–1121.\\n[48] A. A. Nair , T . D. Tran, A. Reiter , and M. A. L. Bell,\\n“A generative adversarial neural network for\\nbeamforming ultrasound images: Invitedpresentation,” in Proc. 53rd Annu. Conf. Inf. Sci.\\nSyst. (CISS) , Mar . 2019, pp. 1–6.\\n[49] B. Luijten et al. , “Deep learning for fast adaptive\\nbeamforming,” in P r o c .I E E EI n t .C o n f .A c o u s t . ,\\nSpeech Signal Process. (ICASSP) , May 2019,\\npp. 1333–1337.\\n[50] S. Boyd and L. Vandenberghe, Convex\\nOptimization . Cambridge, U.K.: Cambridge Univ.\\nPress, 2004.\\n[51] W . Shang, K. Sohn, D. Almeida, and H. Lee,\\n“Understanding and improving convolutional\\nneural networks via concatenated rectiﬁed linearunits,” in Proc. Int. Conf. Mach. Learn. , 2016,\\npp. 2217–2225.\\n[52] F . Gran, A. Jakobsson, and J. A. Jensen, “Adaptivespectral Doppler estimation,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control , vol. 56, no. 4,\\npp. 700–714, Apr . 2009.\\n[53] J. A. Jensen, Estimation of Blood Velocities Using\\nUltrasound: A Signal Processing Approach .\\nCambridge, U.K.: Cambridge Univ. Press, 1996.\\n[54] P . Liu and D. Liu, “Periodically gapped data\\nspectral velocity estimation in medical ultrasound\\nusing spatial and temporal dimensions,” in Proc.\\nIEEE Int. Conf. Acoust., Speech Signal Process. ,\\nApr . 2009, pp. 437–440.\\n[55] R. Cohen and Y. C. Eldar, “Sparse convolutional\\nbeamforming for ultrasound imaging,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control , vol. 65,\\nno. 12, pp. 2390–2406, Dec. 2018.\\n[56] M. W . Rashid et al. , “Front-end electronics for\\ncable reduction in intracardiac echocardiography(ICE) catheters,” in Proc. IEEE Int. Ultrason. Symp.\\n(IUS) , Sep. 2016, pp. 1–4.\\n[ 5 7 ] D .E .D a u s c h ,K .H .G i l c h r i s t ,J .B .C a r l s o n ,\\nS. D. Hall, J. B. Castellucci, and O. T . V . Ramm, “ In\\nvivo real-time 3-D intracardiac echo using PMUT\\narrays,” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 61, no. 10, pp. 1754–1764,\\nOct. 2014.\\n[58] A. Bar-Zion, D. Adam, M. Alessandrini, J. D’hooge,\\nand Y . C Eldar , “Towards sub-Nyquist tissue\\nDoppler imaging using non-uniformly spacedstream of pulses,” in Proc. IEEE Int. Ultrason.\\nSymp. (IUS) , Oct. 2015, pp. 1–4.\\n[59] D. Wildes et al. , “4-D ICE: A 2-D array transducer\\nwith integrated asic in a 10-Fr catheter for\\nreal-time 3-D intracardiac echocardiography ,”\\nIEEE Trans. Ultrason., Ferroelectr ., Freq. Control ,\\nvol. 63, no. 12, pp. 2159–2173, Dec. 2016.\\n[ 6 0 ] D .B e r a ,J .G .B o s c h ,M .D .V e r w e i j ,N .D .J o n g ,\\nand H. J. Vos, “Dual stage beamforming in the\\nabsence of front-end receive focusing,” Phys. Med.\\nBiol. , vol. 62, no. 16, p. 6631, 2017.\\n[61] S. Teerapittayanon, B. McDanel, and H. T . Kung,\\n“Distributed deep neural networks over the cloud,\\nthe edge and end devices,” in Proc. IEEE 37th Int.\\nConf. Distrib. Comput. Syst. (ICDCS) , Jun. 2017,\\npp. 328–339.\\n[ 6 2 ] R .J .V .S l o u n ,H .B e l t ,K .J a n s e ,a n dM .M i s c h i ,\\n“Learning Doppler with deep neural networks and\\nits application to intra-cardiac echography ,” inProc. IEEE Int. Ultrason. Symp. (IUS) , Oct. 2018,\\npp. 1–4.\\n[63] B. Furlow , “Contrast-enhanced ultrasound,”\\nRadiol. Technol. , vol. 80, no. 6, pp. 547S–561S,\\n2009.\\n[64] N. Lassau, L. Chami, B. Benatsou, P . Peronneau,\\nand A. Roche, “Dynamic contrast-enhanced\\nultrasonography (DCE-US) with quantiﬁcation of\\ntumor perfusion: A new diagnostic tool to\\nevaluate the early effects of antiangiogenic\\ntreatment,” Eur . Radiol. Supplements , vol. 17,\\nno. 6, pp. 89–98, 2007.\\n[65] J. M. Hudson et al. , “Dynamic contrast enhanced\\nultrasound for therapy monitoring,” Eur . J.\\nRadiol. , vol. 84, no. 9, pp. 1650–1657, Sep. 2015.\\n[66] T . Opacic et al. , “Motion model ultrasound\\nlocalization microscopy for preclinical and clinical\\nmultiparametric tumor characterization,” Nature\\nCommun. , vol. 9, no. 1, p. 1527, 2018.\\n[67] N. D. Jong, F . T . Cate, C. Lancee, J. Roelandt, and\\nN. Bom, “Principles and recent developments in\\nultrasound contrast agents,” Ultrasonics , vol. 29,\\nno. 4, pp. 324–330, 1991.\\n[ 6 8 ] S .B j æ r u m ,H .T o r p ,a n dK .K r i s t o f f e r s e n ,“ C l u t t e r\\nﬁlter design for ultrasound color ﬂow imaging,”IEEE Trans. Ultrason., Ferroelectr ., Freq. Control ,\\nvol. 49, no. 2, pp. 204–216, Feb. 2002.\\n[69] L. Thomas and A. Hall, “An improved wall ﬁlter for\\nﬂow imaging of low velocity ﬂow ,” in Proc. IEEE\\nUltrason. Symp. , vol. 3, Oct. 1994, pp. 1701–1704.\\n[70] P . J. A. Frinking, A. Bouakaz, J. Kirkhorn, F . J. Ten\\nCate, and N. de Jong, “Ultrasound contrast\\nimaging: Current and new potential methods,”Ultrasound Med. Biol. , vol. 26, no. 6, pp. 965–975,\\nJul. 2000.\\n[71] A. C. Yu and L. Lovstakken, “Eigen-based clutter\\nﬁlter design for ultrasound color ﬂow imaging:\\nAr e v i e w , ” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 57, no. 5, pp. 1096–1111, May 2010.\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 27van Sloun et al. : Deep Learning in Ultrasound Imaging\\n[72] F . W . Mauldin, F . Viola, and W . F . Walker, “Complex\\nprincipal components for robust motion\\nestimation,” IEEE Trans. Ultrason., Ferroelectr .,\\nFreq. Control , vol. 57, no. 11, pp. 2437–2449,\\nNov. 2010.\\n[73] F . W . Mauldin, Jr., D. Lin, and J. A. Hossack, “The\\nsingular value ﬁlter: A general ﬁlter designstrategy for PCA-based signal separation in\\nmedical ultrasound imaging,” IEEE Trans. Med.\\nImag. , vol. 30, no. 11, pp. 1951–1964, Nov. 2011.\\n[74] C. M. Gallippi, K. R. Nightingale, and G. E. Trahey ,\\n“BSS-based ﬁltering of physiological and\\nARFI-induced tissue and blood motion,”\\nUltrasound Med. Biol. , vol. 29, no. 11,\\npp. 1583–1592, 2003.\\n[75] L. Lovstakken, S. Bjaerum, K. Kristoffersen,\\nR. Haaverstad, and H. Torp, “Real-time adaptive\\nclutter rejection ﬁltering in color ﬂow imagingusing power method iterations,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control ,v o l .5 3 ,n o .9 ,\\npp. 1597–1608, Sep. 2006.\\n[76] D. E. Kruse and K. W . Ferrara, “A new high\\nresolution color ﬂow system using an\\neigendecomposition-based adaptive ﬁlter for\\nclutter rejection,” IEEE Trans. Ultrason.,\\nFerroelectr ., Freq. Control , vol. 49, no. 10,\\npp. 1384–1399, Oct. 2002.\\n[77] C. Errico et al. , “Ultrafast ultrasound localization\\nmicroscopy for deep super-resolution vascular\\nimaging,” Nature , vol. 527, no. 7579,\\npp. 499–502, Nov. 2015.\\n[78] P . Song, A. Manduca, J. D. Trzasko, and S. Chen,\\n“Ultrasound small vessel imaging with block-wise\\nadaptive local clutter ﬁltering,” IEEE Trans. Med.\\nImag. , vol. 36, no. 1, pp. 251–262,\\nJan. 2017.\\n[79] A. J. Chee and C. Alfred, “Receiver-operating\\ncharacteristic analysis of eigen-based clutter ﬁlters\\nfor ultrasound color ﬂow imaging,” IEEE Trans.\\nUltrason., Ferroelectr ., Freq. Control ,v o l .6 5 ,n o .3 ,\\npp. 390–399, Mar . 2018.\\n[80] M. Kim, Y. Zhu, J. Hedhli, L. W . Dobrucki, and\\nM. F . Insana, “Multidimensional clutter ﬁlter\\noptimization for ultrasonic perfusion imaging,”\\nIEEE Trans. Ultrason., Ferroelectr ., Freq. control ,\\nvol. 65, no. 11, pp. 2020–2029, Nov. 2018.\\n[81] O. Solomon et al. , “Deep unfolded robust PCA\\nwith application to clutter suppression in\\nultrasound,” 2018, arXiv:1811.08252 .[ O n l i n e ] .\\nAvailable: https://arxiv.org/abs/1811.08252\\n[ 8 2 ] M .A s h i k u z z a m a n ,C .B e l a s s o ,C .G a u t h i e r ,a n d\\nH. Rivaz, “Suppressing clutter components in\\nultrasound color ﬂow imaging using robust matrix\\ncompletion algorithm: Simulation and phantomstudy ,” in Proc. IEEE Int. Symp. Biomed. Imag.\\n(ISBI) , Apr . 2019, pp. 1–4.\\n[83] M. Bayat and M. Fatemi, “Concurrent clutter and\\nnoise suppression via low rank plus sparse\\noptimization for non-contrast ultrasound ﬂow\\nDoppler processing in microvasculature,” in Proc.\\nIEEE Int. Conf. Acoust., Speech Signal Process.\\n(ICASSP) , Apr . 2018, pp. 1080–1084.\\n[ 8 4 ] Y .L i ,M .T o ﬁ g h i ,J .G e n g ,V .M o n g a ,a n d\\nY. C. Eldar, “Deep algorithm unrolling for blind\\nimage deblurring,” 2019, arXiv:1902.03493 .\\n[Online]. Available:\\nhttps://arxiv.org/abs/1902.03493\\n[85] R. Otazo, E. J. Candès, and D. K. Sodickson,\\n“Low-rank plus sparse matrix decomposition foraccelerated dynamic MRI with separation of\\nbackground and dynamic components,” Magn.\\nReson. Med. , vol. 73, no. 3, pp. 1125–1136, 2014.\\n[86] O. Solomon, Y . C. Eldar , M. Mutzaﬁ, and M. Segev,\\n“SPARCOM: Sparsity based super-resolution\\ncorrelation microscopy ,” SIAM J. Imag. Sci. ,\\nvol. 12, no. 1, pp. 392–419, 2019.\\n[87] A. Beck and M. Teboulle, “A fast iterative\\nshrinkage-thresholding algorithm for linear\\ninverse problems,” SIAM J. Imag. Sci. ,v o l .2 ,\\nno. 1, pp. 183–202, 2009.\\n[88] K. Gregor and Y. LeCun, “Learning fast\\napproximations of sparse coding,” in Proc. 27th\\nInt. Conf. Int. Conf. Mach. Learn. , 2010,\\npp. 399–406.[89] Y. LeCun, Y. Bengio, and G. Hinton, “Deep\\nlearning,” Nature , vol. 521, no. 7553, p. 436,\\n2015.\\n[90] A. Paszke et al. , “ Automatic differentiation in\\nPyTorch,” in Proc. 31st Conf. Neural Inf. Process.\\nSyst. (NIPS) , Long Beach, CA, USA, 2017.\\n[91] M. Siepmann, G. Schmitz, J. Bzyl, M. Palmowski,\\nand F . Kiessling, “Imaging tumor vascularity by\\ntracing single microbubbles,” in Proc. IEEE Int.\\nUltrason. Symp. , Oct. 2011, pp. 1906–1909.\\n[92] O. Couture, B. Besson, G. Montaldo, M. Fink, and\\nM. Tanter , “Microbubble ultrasound\\nsuper-localization imaging (MUSLI),” in Proc. IEEE\\nInt. Ultrason. Symp. , Oct. 2011, pp. 1285–1287.\\n[93] O. Couture, V . Hingot, B. Heiles, P .M u l e k i - S e y a ,\\nand M. Tanter, “Ultrasound localization\\nmicroscopy and super-resolution: A state of theart,” IEEE Trans. Ultrason., Ferroelectr ., Freq.\\nControl , vol. 65, no. 8, pp. 1304–1320, Aug. 2018.\\n[ 9 4 ] V .H i n g o t ,C .E r r i c o ,B .H e i l e s ,L .R a h a l ,M .T a n t e r ,\\nand O. Couture, “Microvascular ﬂow dictates the\\ncompromise between spatial resolution and\\nacquisition time in ultrasound localization\\nmicroscopy ,” Sci. Rep. , vol. 9, no. 1, p. 2456, 2019.\\n[95] R. J. V . Sloun et al. , “Super-resolution ultrasound\\nlocalization microscopy through deep learning,”\\n2018, arXiv:1804.07661 . [Online]. Available:\\nhttps://arxiv.org/abs/1804.07661\\n[96] R. J. V . Sloun, O. Solomon, M. Bruce, Z. Z. Khaing,\\nY . C. Eldar, and M. Mischi, “Deep learning for\\nsuper-resolution vascular ultrasound imaging,” in\\nP r o c .I E E EI n t .C o n f .A c o u s t . ,S p e e c hS i g n a lP r o c e s s .(ICASSP) , May 2019, pp. 1055–1059.\\n[97] R. J. V . Sloun, O. Solomon, Y . C. Eldar , H. Wijkstra,\\nand M. Mischi, “Sparsity-driven super-resolution\\nin clinical contrast-enhanced ultrasound,” in Proc.\\nIEEE Int. Ultrason. Symp. (IUS) , Sep. 2017,\\npp. 1–4.\\n[98] S. F . Cotter , B. D. Rao, K. Engan, and\\nK. Kreutz-Delgado, “Sparse solutions to linear\\ninverse problems with multiple measurement\\nvectors,” IEEE Trans. Signal Process. ,v o l .5 3 ,n o .7 ,\\npp. 2477–2488, Jul. 2005.\\n[99] O. Solomon, R. J. V . Sloun, H. Wijkstra, M. Mischi,\\na n dY .C .E l d a r ,“ E x p l o i t i n gﬂ o wd y n a m i c sf o r\\nsuper-resolution in contrast-enhanced\\nultrasound,” 2018, arXiv:1804.03134 .[ O n l i n e ] .\\nAvailable: https://arxiv.org/abs/1804.03134\\n[100] B. Xu, N. Wang, T . Chen, and M. Li, “Empirical\\nevaluation of rectiﬁed activations in convolutional\\nnetwork,” 2015, arXiv:1505.00853 .[ O n l i n e ] .\\nAvailable: https://arxiv.org/abs/1505.00853\\n[101] E. Nehme, L. E. Weiss, T . Michaeli, and\\nY. Shechtman, “Deep-STORM: Super-resolution\\nsingle-molecule microscopy by deep learning,”\\nOptica , vol. 5, no. 4, pp. 458–464, 2018.\\n[102] Z. Z. Khaing et al.\\n, “Contrast-enhanced ultrasound\\nto visualize hemodynamic changes after rodent\\nspinal cord injury ,” J. Neurosurgery Spine ,v o l .2 9 ,\\nno. 3, pp. 306–313, 2018.\\n[103] X.-P . Zhang, “Thresholding neural network for\\nadaptive noise reduction,” IEEE Trans. Neural\\nNetw. , vol. 12, no. 3, pp. 567–584,\\nMay 2001.\\n[104] S. Liu et al. , “Deep learning in medical ultrasound\\nanalysis: A review ,” Engineering ,v o l .5 ,n o .2 ,\\npp. 261–275, Apr . 2019.\\n[105] C. Baumgartner et al. ,“ S o n o N e t :R e a l - t i m e\\ndetection and localisation of fetal standard scan\\nplanes in freehand ultrasound,” IEEE Trans. Med.\\nImag. , vol. 36, no. 11, pp. 2204–2215, Nov. 2017.\\n[106] A. Madani, R. Arnaout, M. Mofrad, and\\nR. Arnaout, “Fast and accurate view classiﬁcation\\nof echocardiograms using deep learning,” NPJ\\nDigit. Med. , vol. 1, no. 1, p. 6, 2018.\\n[107] A. Østvik, E. Smistad, S. A. Aase, B. O. Haugen,\\nand L. Lovstakken, “Real-time standard view\\nclassiﬁcation in transthoracic echocardiography\\nusing convolutional neural networks,” Ultrasound\\nMed. Biol. , vol. 45, no. 2, pp. 374–384, 2019.\\n[108] A. Østvik, E. Smistad, T . Espeland, E. A. R. Berg,\\nand L. Lovstakken, “Automatic myocardial strainimaging in echocardiography using deep\\nlearning,” in Deep Learning in Medical ImageAnalysis and Multimodal Learning for Clinical\\nDecision Support. DLMIA, ML-CDS (Lecture Notes\\nin Computer Science), vol. 11045, D. Stoyanov et\\nal., Eds. Cham, Switzerland: Springer , 2018,\\npp. 309–316.\\n[109] W . Song et al. , “Multitask cascade convolution\\nneural networks for automatic thyroid noduledetection and recognition,” IEEE J. Biomed. Health\\nInformat. , vol. 23, no. 3, pp. 1215–1224,\\nMay 2019.\\n[110] T .-C. Chiang, Y .-S. Huang, R.-T . Chen, C.-S. Huang,\\nand R.-F . Chang, “Tumor detection in automated\\nbreast ultrasound using 3-D CNN and prioritized\\ncandidate aggregation,” IEEE Trans. Med. Imag. ,\\nvol. 38, no. 1, pp. 240–249, Jan. 2019.\\n[111] S. Y . Shin, S. Lee, I. D. Yun, S. M. Kim, and\\nK. M. Lee, “ Joint weakly and semi-supervised deep\\nlearning for localization and classiﬁcation ofmasses in breast ultrasound images,” IEEE Trans.\\nMed. Imag. , vol. 38, no. 3, pp. 762–774,\\nMar . 2018.\\n[112] M. Xian, Y . Zhang, H. D. Cheng, F . Xu, B. Zhang,\\nand J. Ding, “Automatic breast ultrasound image\\nsegmentation: A survey ,” Pattern Recognit. , vol. 79,\\npp. 340–355, Jul. 2018.\\n[113] R. J. G. van Sloun and L. Demi, “Deep learning for\\nautomated detection of b-lines in lung\\nultrasonography ,” J. Acoust. Soc. Amer . , vol. 145,\\nno. 3, p. 1674, 2019.\\n[114] R. J. V . Sloun et al. , “Deep learning for real-time,\\nautomatic, and scanner-adapted prostate (Zone)\\nsegmentation of transrectal ultrasound, for\\nexample, magnetic resonance imaging–transrectal\\nultrasound fusion prostate biopsy ,” Eur . Urol.\\nFocus ,t ob ep u b l i s h e d .d o i :\\n10.1016/j.euf.2019.04.009.\\n[115] Y . Hu et al. , “Weakly-supervised convolutional\\nneural networks for multimodal image\\nregistration,” Med. Image Anal. , vol. 49, pp. 1–13,\\nOct. 2018.\\n[116] C. Hoerig, J. Ghaboussi, and M. F . Insana, “ An\\ninformation-based machine learning approach to\\nelasticity imaging,” Biomech. Model. Mechanobiol. ,\\nvol. 16, no. 3, pp. 805–822, 2017.\\n[117] C. Hoerig, J. Ghaboussi, and M. F . Insana,\\n“Data-driven elasticity imaging using cartesian\\nneural network constitutive models and the\\nautoprogressive method,” IEEE Trans. Med. Imag. ,\\nvol. 38, no. 5, pp. 1150–1160, May 2018.\\n[118] M. Feigin, D. Freedman, and B. W . Anthony ,\\n“A deep learning framework for single-sided\\nsound speed inversion in medical ultrasound,”\\n2018, arXiv:1810.00322 . [Online]. Available:\\nhttps://arxiv.org/abs/1810.00322\\n[119] V . Vishnevskiy , S. J. Sanabria, and O. Goksel,\\n“Image reconstruction via variational network forreal-time hand-held sound-speed imaging,” in\\nMachine Learning for Medical Image\\nReconstruction. MLMIR (Lecture Notes in\\nComputer Science), vol. 11074, F . Knoll, A. Maier ,\\nand D. Rueckert, Eds. Cham, Switzerland:\\nSpringer , 2018, pp. 120–128.\\n[120] J. Johansson, M. Gustafsson, and J. Delsing,\\n“Ultra-low power transmit/receive ASIC for\\nbattery operated ultrasound measurementsystems,” Sens. Actuators A, Phys. , vol. 125, no. 2,\\npp. 317–328, 2006.\\n[121] N. Jouppi, C. Young, N. Patil, and D. Patterson,\\n“Motivation for and evaluation of the ﬁrst tensorprocessing unit,” IEEE Micro ,v o l .3 8 ,n o .3 ,\\npp. 10–19, May/Jun. 2018.\\n[122] G. Hinton, O. Vinyals, and J. Dean, “Distilling the\\nknowledge in a neural network,” 2015,arXiv:1503.02531 . [Online]. Available:\\nhttps://arxiv.org/abs/1503.02531\\n[123] I. Hubara, M. Courbariaux, D. Soudry , R. El-Yaniv,\\nand Y. Bengio, “Quantized neural networks:\\nTraining neural networks with low precision\\nweights and activations,” J. Mach. Learn. Res. ,\\nvol. 18, no. 1, pp. 6869–6898, 2017.\\n[124] P . Sermanet et al. , “Time-contrastive networks:\\nSelf-supervised learning from video,” in Proc. IEEE\\nInt. Conf. Robot. Autom. (ICRA) , May 2018,\\npp. 1134–1141.\\n28 PROCEEDINGS OF THE IEEE | Vol. 108, No. 1, January 2020van Sloun et al. : Deep Learning in Ultrasound Imaging\\nABOUT THE AUTHORS\\nRuud J. G. van Sloun (Member, IEEE)\\nreceived the B.Sc. and M.Sc. degrees (cum\\nlaude) in electrical engineering and the\\nPh.D. degree (cum laude) from the Eind-\\nhoven University of Technology, Eindhoven,The Netherlands, in 2012, 2014, and 2018,\\nrespectively.\\nSince then, he has been an Assistant\\nProfessor with the Department of ElectricalEngineering, Eindhoven University of Technology. He is also a Vis-iting Professor with the Department of Mathematics and ComputerScience, Weizmann Institute of Science, Rehovot, Israel. His currentresearch interests include artiﬁcial intelligence and deep learn-ing for front-end signal processing, model-aware deep learning,compressed sensing, ultrasound imaging, and signal and imageanalysis.\\nRegev Cohen (Graduate Student Member,\\nIEEE) received the B.Sc. degree (summa\\ncum laude) in electrical engineering from\\nthe Technion—Israel Institute of Technology,Haifa, Israel, in 2015, where he is currentlyworking toward the Ph.D. degree.\\nHis current research interests include\\ntheoretical aspects of signal processing,sampling theory, compressed sensing, opti-mization methods, sparse array design, deep learning, andadvanced signal processing methods for ultrasonic imaging.\\nMr. Cohen received the Meyer Foundation Excellence Award and\\nthe Elias-Perlmutter Award in 2015. In 2017, he was awarded theIsrael and Debora Cederbaum Scholarship.\\nYonina C. Eldar (Fellow, IEEE) received the\\nB.Sc. degree in physics and the B.Sc. degreein electrical engineering from Tel Aviv Uni-versity (TAU), Tel Aviv, Israel, in 1995 and1996, respectively, and the Ph.D. degree inelectrical engineering and computer sciencefrom the Massachusetts Institute of Technol-ogy (MIT), Cambridge, MA, USA, in 2002.\\nShe was a Professor with the Department\\nof Electrical Engineering, Technion, Haifa, Israel, where she heldthe Edwards Chair in Engineering. She was a Visiting Professorwith Stanford University, Stanford, CA, USA. She is currently aProfessor with the Department of Mathematics and ComputerScience, Weizmann Institute of Science, Rehovot, Israel. She isalso a Visiting Professor with MIT, a Visiting Scientist with the\\nBroad Institute, Cambridge, MA, USA, and an Adjunct Professor with\\nDuke University, Durham, NC, USA. She is the author of the bookSampling Theory: Beyond Bandlimited Systems and a coauthor of\\nthe books Compressed Sensing andConvex Optimization Methods\\nin Signal Processing and Communications (Cambridge University\\nPress). Her current research interests include the broad areasof statistical signal processing, sampling theory and compressedsensing, learning and optimization methods, and their applicationsto medical imaging, biology, and optics.\\nDr. Eldar was a member of the Young Israel Academy of Science\\nand Humanities and the Israel Committee for Higher Education.She was a member of the IEEE Signal Processing Theory andMethods and Bio Imaging Signal Processing Technical Committees.She was a Horev Fellow of the Leaders in Science and TechnologyProgram at the Technion and an Alon Fellow. She is also a memberof the IEEE Sensor Array and Multichannel Technical Committee andserves on several other IEEE committees. She is also a member ofthe Israel Academy of Sciences and Humanities (elected in 2017)and a EURASIP Fellow. She received many awards for excellencein research and teaching, including the IEEE Signal ProcessingSociety Technical Achievement Award in 2013, the IEEE/AESS FredNathanson Memorial Radar Award in 2014, and the IEEE KiyoTomiyasu Award in 2016. She also received the Michael BrunoMemorial Award from the Rothschild Foundation, the WeizmannPrize for Exact Sciences, the Wolf Foundation Krill Prize for Excel-lence in Scientiﬁc Research, the Henry Taub Prize for Excellence inResearch (twice), the Hershel Rich Innovation Award (three times),the Award for Women with Distinguished Contributions, the Andreand Bella Meyer Lectureship, the Career Development Chair atthe Technion, the Muriel & David Jacknow Award for Excellencein Teaching, and the Technion’s Award for Excellence in Teaching(two times). She also received several best paper awards and bestdemo awards together with her research students and colleagues,including the SIAM Outstanding Paper Prize, the UFFC OutstandingPaper Award, the Signal Processing Society Best Paper Award, andthe IET Circuits, Devices and Systems Premium Award. She wasselected as one of the 50 most inﬂuential women in Israel. Shewas the co-chair and the technical co-chair of several internationalconferences and workshops. She was a Signal Processing SocietyDistinguished Lecturer. She has served as an Associate Editor forthe IEEE T\\nRANSACTIONS ON SIGNAL PROCESSING ,EURASIP Journal on\\nAdvances in Signal Processing ,SIAM Journal on Matrix Analysis and\\nApplications ,a n d SIAM Journal on Imaging Sciences .S h ei sa l s ot h e\\nEditor-in-Chief of Foundations and Trends in Signal Processing .\\nVol. 108, No. 1, January 2020 |PROCEEDINGS OF THE IEEE 29'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"Deep_Learning_in_Ultrasound.pdf\")\n",
    "\n",
    "raw_text = \"\"\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "raw_text[:10000000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This article by Ruud J. G. van Sloun, Regev Cohen, and Yonina C. Eldar discusses the use of deep learning in ultrasound imaging, from beamforming to advanced applications such as elastography, acoustic radiation force impulse imaging, and shear wave elastography imaging. It covers topics such as adaptive beamforming, adaptive spectral Doppler, compressive tissue Doppler, and clutter suppression, as well as applications such as automated detection and recognition of fetal standard scan planes, myocardial strain imaging, tumor detection, and localization and classification of masses in breast ultrasound images. It also looks at the use of deep learning for super-resolution microvascular ultrasound imaging, contrast-enhanced ultrasound, and ultrasound localization microscopy.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_document_chain.run(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\") # gpt-3.5-turbo, gpt-4\n",
    "\n",
    "qa_chain = load_qa_chain(model, chain_type=\"map_reduce\")\n",
    "qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-VyQlTknAQEhKzUp32AphKtcY on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m qa_document_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m      2\u001b[0m     input_document\u001b[39m=\u001b[39;49mraw_text,\n\u001b[0;32m      3\u001b[0m     question\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m딥러닝을 어떻게 적용했나\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\base.py:216\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    218\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but not both. Got args: \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m and kwargs: \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:117\u001b[0m, in \u001b[0;36mAnalyzeDocumentChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    115\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m    116\u001b[0m other_keys[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs_chain\u001b[39m.\u001b[39minput_key] \u001b[39m=\u001b[39m docs\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs_chain(other_keys, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:75\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m     74\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m---> 75\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(docs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys)\n\u001b[0;32m     76\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m     77\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:139\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m, docs: List[Document], token_max: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \n\u001b[0;32m    136\u001b[0m \u001b[39m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mapply(\n\u001b[0;32m    140\u001b[0m         \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m         [{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_variable_name: d\u001b[39m.\u001b[39;49mpage_content}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs} \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m docs]\n\u001b[0;32m    142\u001b[0m     )\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_results(results, docs, token_max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[0;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chains\\llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[1;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\base.py:82\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m---> 82\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\base.py:79\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m     76\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompt_strings, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m     77\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m     80\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\base.py:54\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m, messages: List[List[BaseMessage]], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     52\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Top Level call\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     results \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(m, stop\u001b[39m=\u001b[39mstop) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[0;32m     55\u001b[0m     llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[0;32m     56\u001b[0m     generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\base.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m, messages: List[List[BaseMessage]], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     52\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Top Level call\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     results \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(m, stop\u001b[39m=\u001b[39;49mstop) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m messages]\n\u001b[0;32m     55\u001b[0m     llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n\u001b[0;32m     56\u001b[0m     generations \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39mgenerations \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\openai.py:266\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop)\u001b[0m\n\u001b[0;32m    262\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    263\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion, \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: role}\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[1;32m--> 266\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(messages\u001b[39m=\u001b[39;49mmessage_dicts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\openai.py:228\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py:432\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py:388\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_result\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m--> 388\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    390\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\langchain\\chat_models\\openai.py:226\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    514\u001b[0m     _thread_context\u001b[39m.\u001b[39msession \u001b[39m=\u001b[39m _make_session()\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    527\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py:1322\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1321\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1322\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1323\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py:303\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    304\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    305\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\http\\client.py:264\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 264\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\sel04327\\AppData\\Local\\Programs\\Python\\Python38\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa_document_chain.run(\n",
    "    input_document=raw_text,\n",
    "    question=\"딥러닝을 어떻게 적용했나\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
